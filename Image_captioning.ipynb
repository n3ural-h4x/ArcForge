{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/myprojectishere/myproject/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.78s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.03s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import CLIPTokenizer  \n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "class CocoImageCaptionDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_file, transform=None, max_length=50):\n",
    "        self.image_dir = image_dir\n",
    "        self.coco = COCO(annotation_file)\n",
    "        self.transform = transform\n",
    "        # Changed: Use CLIP tokenizer for caption embedding\n",
    "        self.tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Get image IDs and their captions\n",
    "        self.image_ids = list(self.coco.imgs.keys())\n",
    "        self.annotations = {img_id: [] for img_id in self.image_ids}\n",
    "        \n",
    "        for ann in self.coco.loadAnns(self.coco.getAnnIds()):\n",
    "            self.annotations[ann['image_id']].append(ann['caption'])\n",
    "        if self.tokenizer.pad_token is None or self.tokenizer.pad_token == self.tokenizer.eos_token:\n",
    "             self.tokenizer.add_special_tokens({'pad_token': '<pad>'})\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get image ID\n",
    "        image_id = self.image_ids[idx]\n",
    "\n",
    "        # Load image\n",
    "        img_info = self.coco.loadImgs(image_id)[0]\n",
    "        img_path = os.path.join(self.image_dir, img_info['file_name'])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        # Apply transforms if available\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Choose a random caption for this image\n",
    "        captions = self.annotations[image_id]\n",
    "        caption = captions[torch.randint(0, len(captions), (1,)).item()]\n",
    "        \n",
    "        # Tokenize caption with CLIP's tokenizer\n",
    "        encoding = self.tokenizer(\n",
    "            caption,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=False\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "\n",
    "        return image, input_ids\n",
    "\n",
    "# Image transformations\n",
    "# Changed: Updated normalization values to those typically used for CLIP\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073], \n",
    "                         std=[0.26862954, 0.26130258, 0.27577711]),\n",
    "])\n",
    "\n",
    "# Dataset paths\n",
    "train_image_dir = '/root/myprojectishere/CocoDataset/train2017'\n",
    "train_annotation_file = '/root/myprojectishere/CocoDataset/annotations/captions_train2017.json'\n",
    "\n",
    "val_image_dir = '/root/myprojectishere/CocoDataset/val2017'\n",
    "val_annotation_file = '/root/myprojectishere/CocoDataset/annotations/captions_val2017.json'\n",
    "\n",
    "# Initialize datasets\n",
    "dataset_train = CocoImageCaptionDataset(\n",
    "    image_dir=train_image_dir,\n",
    "    annotation_file=train_annotation_file,\n",
    "    transform=transform,\n",
    "    max_length=50\n",
    ")\n",
    "\n",
    "dataset_val = CocoImageCaptionDataset(\n",
    "    image_dir=val_image_dir,\n",
    "    annotation_file=val_annotation_file,\n",
    "    transform=transform,\n",
    "    max_length=50\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=64, shuffle=True, num_workers=12,pin_memory=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=32, shuffle=False, num_workers=12,pin_memory=True)\n",
    "\n",
    "# Test iteration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torchvision.models as models\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.checkpoint import checkpoint, checkpoint_sequential\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class RotaryPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, d_embed, base=10000):\n",
    "        super().__init__()\n",
    "        self.d_embed = d_embed\n",
    "        self.base = base\n",
    "\n",
    "        # Create the frequencies for rotary embeddings\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, d_embed, 2).float() / d_embed))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "    def forward(self, seq_len,device):\n",
    "        t = torch.arange(seq_len, dtype=torch.float32,device=device)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        return torch.cat((freqs.sin(), freqs.cos()), dim=-1)\n",
    "\n",
    "def apply_rotary_embedding(x, rotary_emb):\n",
    "    # x Batch_Size, H, Seq_Len, D_Head\n",
    "    seq_len = x.size(-2)\n",
    "    rotary_emb = rotary_emb[:seq_len, :]\n",
    "\n",
    "    rotary_emb = rotary_emb.unsqueeze(0).unsqueeze(0)  # (1, 1, Seq_Len, D_Head)\n",
    "\n",
    "    x_1, x_2 = torch.chunk(x, 2 , dim=-1)\n",
    "    emb_sin, emb_cos = torch.chunk(rotary_emb, 2, dim=-1)\n",
    "\n",
    "    x_rot = x_1 * emb_cos - x_2 * emb_sin\n",
    "    x_pass = x_1 * emb_sin + x_2 * emb_cos\n",
    "\n",
    "    return torch.cat([x_rot, x_pass], dim=-1)\n",
    "\n",
    "# Updated SelfAttention with Rotary Position Embeddings\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_embed, in_proj_bias=True, out_proj_bias=True):\n",
    "        super().__init__()\n",
    "        self.in_proj = nn.Linear(d_embed, 3 * d_embed, bias=in_proj_bias)\n",
    "        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_embed // n_heads\n",
    "\n",
    "        # Rotary Position Embeddings\n",
    "        self.rotary_emb = RotaryPositionEmbeddings(self.d_head)\n",
    "\n",
    "    def forward(self, x, attention_mask:None, causal_mask=False):\n",
    "        batch_size, seq_len, d_embed = x.size()\n",
    "        dtype = x.dtype\n",
    "        q, k, v = self.in_proj(x).chunk(3, dim=-1)\n",
    "\n",
    "        # Reshape to multi-head shape\n",
    "        q = q.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "\n",
    "        # Apply rotary position embeddings\n",
    "        rotary_emb = self.rotary_emb(seq_len, x.device)\n",
    "        q = apply_rotary_embedding(q, rotary_emb)\n",
    "        k = apply_rotary_embedding(k, rotary_emb)\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = q @ k.transpose(-1, -2) / math.sqrt(self.d_head)\n",
    "\n",
    "\n",
    "        if attention_mask is not None:\n",
    "        # attention_mask: (batch, seq_len) -> (batch, 1, 1, seq_len)\n",
    "         extended_mask = attention_mask[:, None, None, :].to(dtype=attn_weights.dtype) \n",
    "         attn_weights = attn_weights.masked_fill(extended_mask == 0, float('-inf'))\n",
    "\n",
    "        if causal_mask:\n",
    "            mask = torch.ones_like(attn_weights, dtype=torch.bool).triu(1)\n",
    "            attn_weights.masked_fill_(mask, float('-inf'))\n",
    "\n",
    "        attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1,dtype=torch.float32).to(dtype)\n",
    "        \n",
    "        # Compute attention output\n",
    "        attn_output = attn_weights @ v\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_embed)\n",
    "        return self.out_proj(attn_output)\n",
    "\n",
    "# Updated CrossAttention with Rotary Position Embeddings\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_embed, d_cross, in_proj_bias=True, out_proj_bias=True):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(d_embed, d_embed, bias=in_proj_bias)\n",
    "        self.k_proj = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
    "        self.v_proj = nn.Linear(d_cross, d_embed, bias=in_proj_bias)\n",
    "        self.out_proj = nn.Linear(d_embed, d_embed, bias=out_proj_bias)\n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_embed // n_heads\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        batch_size, seq_len_q, d_embed = x.size()\n",
    "        dtype=x.dtype\n",
    "        if len(y.size()) == 2:\n",
    "         y=y.unsqueeze(1)\n",
    "         seq_len_kv=y.size(1)\n",
    "        else:\n",
    "         seq_len_kv=y.size(1)\n",
    "\n",
    "        q = self.q_proj(x).view(batch_size, seq_len_q, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        k = self.k_proj(y).view(batch_size, seq_len_kv, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        v = self.v_proj(y).view(batch_size, seq_len_kv, self.n_heads, self.d_head).transpose(1, 2)\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = q @ k.transpose(-1, -2) / math.sqrt(self.d_head)\n",
    "        attn_weights = F.softmax(attn_weights,dim=-1,dtype=torch.float32).to(dtype)\n",
    "\n",
    "        # Compute attention output\n",
    "        attn_output = attn_weights @ v\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len_q, d_embed)\n",
    "        return self.out_proj(attn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Without Causal Mask ===\n",
      "Output shape: torch.Size([2, 6, 64])\n",
      "\n",
      "=== With Causal Mask ===\n",
      "Output shape: tensor([[[-2.2987e-02,  5.4691e-01,  3.9977e-02,  1.4782e-02, -2.7799e-02,\n",
      "          -2.5005e-01,  6.4767e-01, -1.7515e-01, -1.4223e-01, -1.8354e-01,\n",
      "           2.3692e-01, -1.4711e-01,  2.8328e-01,  5.7776e-02,  1.4011e-01,\n",
      "           6.6510e-03, -3.0640e-01, -1.1216e-01, -2.9874e-01, -1.0162e-01,\n",
      "           2.0155e-01, -5.3056e-02,  3.4103e-01,  3.6450e-02, -6.9006e-01,\n",
      "           2.9858e-01, -2.9129e-01, -1.3770e-01,  8.0249e-02,  1.1041e-01,\n",
      "          -6.4675e-01,  6.0678e-01,  9.2376e-02,  1.8472e-01,  1.5306e-01,\n",
      "           1.3724e-01,  3.4186e-01,  4.1708e-01,  6.9922e-01, -5.2091e-02,\n",
      "           8.5425e-02, -3.0541e-02,  1.3735e-02,  3.5371e-01,  4.2012e-01,\n",
      "           1.8507e-01,  3.5497e-01,  7.1163e-01,  5.6964e-01,  3.7103e-01,\n",
      "          -2.6992e-01,  8.0485e-01, -3.9273e-01,  8.4107e-02,  1.7595e-01,\n",
      "          -7.6952e-01,  5.7156e-02,  1.5924e-02,  1.6571e-01, -3.2918e-01,\n",
      "          -1.1343e-01,  6.2486e-01,  4.0195e-02, -4.4504e-02],\n",
      "         [ 2.0198e-01,  2.5183e-01,  2.2673e-01, -1.6641e-01,  4.0123e-03,\n",
      "          -2.7530e-01,  1.4643e-01, -1.1582e-02, -5.4524e-02, -1.0701e-01,\n",
      "           2.6592e-02, -9.4833e-02,  4.7122e-01, -4.7199e-02, -7.9149e-02,\n",
      "           1.5620e-01,  3.3242e-02, -2.9935e-01, -3.5446e-01, -1.0410e-01,\n",
      "           5.5536e-02, -3.0474e-01, -1.9487e-02, -1.4480e-01, -2.2816e-01,\n",
      "          -2.9002e-03, -2.0229e-01,  1.3140e-01,  2.9938e-01,  1.6934e-01,\n",
      "          -2.4867e-01,  1.7382e-01,  1.3558e-01,  1.0989e-01,  2.8576e-01,\n",
      "          -5.1106e-02, -9.5949e-02, -1.8732e-02,  4.2717e-01,  1.0413e-01,\n",
      "          -3.8382e-02, -1.3062e-01, -3.3687e-01,  3.6653e-01,  2.6995e-01,\n",
      "           5.4478e-01,  5.2716e-01,  1.6325e-01,  2.2744e-01,  3.8496e-01,\n",
      "          -2.3450e-01,  4.7195e-01, -1.5129e-01, -7.0529e-04, -6.1818e-02,\n",
      "          -1.6948e-01,  1.1682e-01,  5.8194e-02,  1.3443e-01, -2.3492e-01,\n",
      "           9.0544e-02,  3.1564e-01, -5.5839e-03, -1.6674e-02],\n",
      "         [-2.8850e-02,  3.3345e-01,  5.3562e-03, -2.1330e-01, -1.0314e-01,\n",
      "          -9.6324e-02,  5.8998e-02, -3.5998e-02, -8.1250e-02,  1.6416e-01,\n",
      "           5.6591e-02,  4.2157e-02,  1.9662e-01, -2.4560e-01, -1.2416e-01,\n",
      "          -5.3130e-02, -8.8934e-02,  4.9465e-02, -1.6196e-01, -8.5485e-02,\n",
      "           1.8085e-01,  2.0232e-02,  1.1255e-01, -1.2072e-01, -1.9207e-01,\n",
      "          -1.5360e-01, -3.5512e-02,  1.0320e-04,  4.2860e-01,  1.8551e-02,\n",
      "          -3.9675e-01, -1.1756e-01,  1.1255e-01,  1.0066e-02,  1.1882e-01,\n",
      "           1.3299e-01, -3.0915e-03,  3.6186e-03,  4.2944e-01, -2.1970e-02,\n",
      "           2.7574e-01,  1.9583e-01, -2.6183e-02,  3.7212e-01,  2.6587e-01,\n",
      "           1.5767e-01,  5.1227e-01, -1.8066e-01, -5.8580e-02,  3.7749e-01,\n",
      "           1.2822e-01,  2.5689e-01, -1.6140e-01,  1.2712e-01, -1.3948e-01,\n",
      "          -1.6390e-01,  2.9358e-01,  2.1179e-01,  6.4826e-02,  5.7814e-02,\n",
      "           1.2996e-01,  4.0411e-01,  6.8733e-03, -1.4074e-01],\n",
      "         [ 4.1106e-02,  2.6589e-01, -2.3591e-02, -6.0693e-02, -5.9914e-02,\n",
      "          -3.8401e-02,  2.2881e-02,  3.5275e-02, -1.9474e-01,  1.2535e-01,\n",
      "           6.4049e-02,  6.2428e-03,  2.7644e-01, -1.6851e-01, -1.0557e-01,\n",
      "          -1.0973e-01,  5.0308e-03,  8.5318e-03, -1.7925e-01, -1.5376e-01,\n",
      "           6.1654e-04,  5.2244e-02,  6.2827e-02, -8.7825e-02, -1.7137e-01,\n",
      "           4.9049e-02, -1.7100e-02, -4.0067e-02,  4.2845e-01,  1.8886e-02,\n",
      "          -2.6200e-01, -9.9576e-02,  1.2440e-01,  1.5791e-01,  1.8510e-01,\n",
      "          -2.2305e-02, -3.7525e-02, -1.4232e-02,  3.1883e-01, -1.7626e-02,\n",
      "           1.4881e-01,  1.1742e-01,  3.0995e-02,  1.4214e-01,  1.9852e-01,\n",
      "           1.5437e-01,  4.1069e-01, -1.3079e-01, -1.3917e-01,  2.7207e-01,\n",
      "           9.3417e-03,  2.2432e-01, -5.6567e-02, -5.1039e-02, -1.1516e-02,\n",
      "          -1.5949e-01,  1.5905e-01,  2.3354e-01,  9.4120e-03,  1.0812e-01,\n",
      "           7.2227e-02,  2.3446e-01, -3.5490e-02, -1.3297e-01],\n",
      "         [ 3.5343e-02,  2.8874e-01, -1.5280e-01,  7.2022e-02,  4.5484e-02,\n",
      "          -2.1579e-02, -1.5731e-01,  1.4689e-01, -1.8188e-01,  2.3199e-01,\n",
      "          -1.0346e-02, -3.5902e-02,  2.9824e-01, -1.4399e-01, -1.5068e-01,\n",
      "          -1.6890e-01,  9.4327e-02,  4.5995e-02, -4.8309e-02, -1.4906e-01,\n",
      "          -1.4991e-01,  1.0034e-01,  4.5889e-02, -5.9824e-02, -1.3011e-01,\n",
      "          -5.3069e-02, -1.4482e-02, -2.5536e-02,  4.5584e-01, -1.0253e-01,\n",
      "          -1.2141e-01, -4.3035e-01,  3.6794e-03,  7.8533e-02,  1.6780e-01,\n",
      "          -9.3842e-02,  3.8546e-02,  6.4479e-02,  2.1322e-01, -7.2795e-02,\n",
      "           7.7310e-02,  2.1676e-01,  1.2852e-01, -8.3823e-02,  6.7371e-02,\n",
      "           9.7205e-02,  3.2943e-01, -1.9761e-01, -2.3909e-01,  3.4334e-01,\n",
      "          -5.4229e-02,  2.9933e-02, -3.4893e-02, -1.2265e-01, -5.8058e-02,\n",
      "           5.1369e-02,  1.9813e-01,  2.4810e-01, -1.3297e-01,  1.7016e-01,\n",
      "           4.9508e-02,  2.1592e-02, -1.2536e-01, -2.7042e-02],\n",
      "         [ 2.6567e-02,  2.6410e-01, -2.0059e-01,  3.2069e-02,  3.9148e-02,\n",
      "          -5.1211e-02, -1.3124e-01,  1.3730e-01, -1.0031e-01,  1.5846e-01,\n",
      "           6.8504e-02, -1.0500e-02,  1.8494e-01, -9.8826e-02, -1.1582e-01,\n",
      "          -2.2650e-01,  2.8370e-03, -5.3875e-02, -1.0291e-01, -8.9816e-02,\n",
      "          -7.2097e-02, -4.6347e-03,  5.0886e-02, -1.1260e-01, -1.7749e-01,\n",
      "          -2.3967e-02, -3.0918e-02, -5.0077e-02,  3.4144e-01, -1.0328e-01,\n",
      "          -8.7206e-02, -3.1025e-01, -7.5318e-03,  1.7891e-02,  1.4736e-01,\n",
      "          -6.0674e-02,  9.3747e-02,  7.6800e-03,  2.4114e-01, -8.2541e-02,\n",
      "           9.8342e-03,  1.3795e-01,  1.4814e-01, -4.1237e-02,  1.1437e-01,\n",
      "           1.1770e-01,  2.2788e-01, -1.1549e-01, -1.8899e-01,  2.5084e-01,\n",
      "           2.5767e-02, -4.5359e-03,  9.0313e-02, -8.5444e-02, -2.5376e-02,\n",
      "           2.3013e-02,  1.5920e-01,  1.0934e-01, -1.0061e-01,  1.5457e-01,\n",
      "           6.5686e-02,  4.8556e-02, -6.6655e-02,  3.0603e-03]],\n",
      "\n",
      "        [[ 3.4094e-01,  2.8615e-01,  2.4709e-01,  2.3055e-01,  3.1231e-01,\n",
      "           2.2551e-01,  2.9499e-01, -5.7793e-02, -1.8679e-01,  1.5711e-01,\n",
      "          -2.5546e-02,  3.9199e-01,  4.9738e-01,  7.7142e-01,  4.8878e-01,\n",
      "           3.5487e-02,  7.8453e-01,  3.8266e-01,  1.1364e-01,  1.2776e-01,\n",
      "           2.1740e-01, -3.3378e-01, -3.2493e-02, -3.0254e-02,  9.3346e-01,\n",
      "          -3.0650e-01, -4.6870e-01, -1.4376e-01, -7.3194e-03,  2.0488e-01,\n",
      "           5.9724e-01, -1.0562e+00,  4.0603e-01, -3.5776e-01,  6.3526e-01,\n",
      "          -1.6458e-01,  2.7107e-01,  3.5544e-01, -4.5292e-02, -1.0863e+00,\n",
      "           7.6431e-02,  5.0224e-01,  4.1783e-01, -7.3840e-02, -7.8347e-02,\n",
      "           5.7281e-02,  4.9712e-01, -1.1843e-01, -1.2883e-01,  7.4613e-01,\n",
      "          -5.9299e-01,  7.1804e-02,  5.0760e-02, -2.0634e-01, -7.1718e-01,\n",
      "           7.7850e-01,  3.7836e-01,  4.7310e-02, -5.1323e-01,  2.4545e-01,\n",
      "          -3.1752e-01, -2.4820e-01, -6.0557e-01, -5.9935e-01],\n",
      "         [ 1.6620e-01, -1.3072e-01,  1.6171e-01,  2.8056e-02, -5.2098e-02,\n",
      "           2.2253e-01, -6.2018e-02, -3.3708e-02, -3.9947e-01,  1.8938e-01,\n",
      "          -3.6709e-01,  6.7867e-02,  1.4087e-01,  4.8081e-01,  3.5130e-02,\n",
      "          -1.6652e-01,  4.8276e-01,  1.3458e-02,  2.5908e-02,  8.6502e-02,\n",
      "           2.4279e-01, -4.1858e-01, -9.2613e-02, -4.3974e-01,  4.1065e-01,\n",
      "          -2.8616e-01, -7.2806e-02, -1.7165e-01, -1.0260e-01,  3.9148e-01,\n",
      "           2.7047e-01, -6.3213e-01,  4.0794e-01, -1.0138e-01,  5.6871e-01,\n",
      "          -8.5840e-02, -2.7706e-02,  4.6981e-02, -7.1971e-02, -5.5469e-01,\n",
      "          -1.2712e-01,  3.5069e-01,  5.6912e-01, -1.5457e-01, -1.3455e-01,\n",
      "          -2.0646e-01,  5.4640e-01, -2.4918e-01, -4.5697e-01,  2.1985e-01,\n",
      "          -1.3173e-01, -1.6338e-01,  2.2437e-01, -1.1181e-01, -4.2123e-01,\n",
      "           5.3214e-01,  5.3746e-01, -5.2949e-02, -2.7429e-01,  2.1981e-01,\n",
      "          -1.5554e-01, -3.5693e-01, -1.5242e-01, -1.5703e-01],\n",
      "         [ 9.9790e-02,  9.8709e-02,  5.7589e-02,  2.7789e-01,  1.6258e-01,\n",
      "           3.2049e-01, -2.0558e-03,  2.4663e-02, -4.0553e-01, -2.8313e-02,\n",
      "          -1.0804e-01, -3.1435e-02,  1.7919e-01,  1.7392e-01,  3.9389e-01,\n",
      "          -1.6692e-01,  4.6139e-01,  2.9779e-01,  1.3090e-01,  2.8435e-01,\n",
      "           1.8659e-01, -4.1085e-02,  1.4684e-02, -2.4447e-01,  3.4996e-01,\n",
      "          -1.2816e-01,  1.1186e-01, -2.9626e-01, -1.3099e-01,  1.8277e-01,\n",
      "           2.1309e-01, -5.7598e-01,  4.3647e-01, -1.4247e-01,  2.2313e-01,\n",
      "           1.3172e-01,  2.7502e-01,  5.3473e-02,  1.1814e-01, -3.6281e-01,\n",
      "           1.1544e-01,  1.6514e-01,  4.7992e-01, -1.5818e-01, -2.9189e-01,\n",
      "           1.9144e-02,  3.6473e-01, -1.0677e-01, -2.2933e-01,  2.9853e-01,\n",
      "          -1.1838e-01, -4.9150e-02,  1.4641e-02, -9.6861e-02, -4.8548e-01,\n",
      "           3.4268e-01,  2.0964e-01,  1.3168e-01, -1.8332e-01,  2.4301e-01,\n",
      "          -2.4481e-02, -1.8935e-01, -3.7001e-01, -3.2243e-01],\n",
      "         [ 1.1662e-01,  1.4006e-02,  1.2487e-02,  2.8763e-01, -1.7668e-02,\n",
      "           1.9295e-01, -5.6787e-02, -3.7972e-02, -3.0728e-01,  6.1339e-02,\n",
      "          -2.3605e-01, -2.0484e-02,  2.0932e-01,  3.0980e-01,  1.4570e-01,\n",
      "          -1.7018e-01,  3.6602e-01,  1.8538e-01,  1.0423e-01,  2.1286e-01,\n",
      "           1.9109e-01, -1.6218e-01, -1.2481e-02, -3.3228e-01,  2.2746e-01,\n",
      "          -1.9347e-01,  2.0391e-02, -3.7004e-01, -1.2373e-01,  1.8556e-01,\n",
      "           2.7541e-01, -6.9455e-01,  4.2791e-01, -1.0288e-01,  3.5235e-01,\n",
      "          -3.2513e-02,  2.2888e-01,  7.9738e-02, -4.8163e-03, -3.9533e-01,\n",
      "           7.2648e-02,  1.8392e-01,  5.5194e-01, -2.8499e-01, -2.3643e-01,\n",
      "           2.0837e-02,  4.4118e-01, -2.3183e-01, -2.5817e-01,  2.6470e-01,\n",
      "          -1.8372e-01, -1.3840e-01,  3.3567e-03, -7.4057e-02, -4.1402e-01,\n",
      "           3.9236e-01,  3.3818e-01,  5.6567e-02, -2.5442e-01,  2.5991e-01,\n",
      "          -1.1994e-01, -2.1935e-01, -3.3215e-01, -3.3190e-01],\n",
      "         [ 1.1332e-01,  9.6032e-02,  6.9926e-02,  2.5526e-01,  9.8168e-02,\n",
      "           2.6955e-01, -2.1638e-02,  6.5954e-03, -3.4057e-01,  6.5973e-02,\n",
      "          -1.8822e-01, -1.4234e-02,  1.1332e-01,  2.9930e-01,  2.1591e-01,\n",
      "          -1.3215e-01,  4.4005e-01,  1.8720e-01,  6.7411e-02,  2.4043e-01,\n",
      "           1.4072e-01, -1.5890e-01,  5.2127e-02, -3.3091e-01,  2.9405e-01,\n",
      "          -1.7665e-01,  7.6602e-02, -2.9495e-01, -8.5879e-02,  2.1615e-01,\n",
      "           3.0119e-01, -6.6528e-01,  3.7931e-01, -6.9569e-02,  3.4072e-01,\n",
      "           5.5092e-02,  1.8972e-01,  1.2957e-01,  5.8947e-02, -4.4496e-01,\n",
      "           5.8591e-02,  3.1719e-01,  4.4570e-01, -2.0611e-01, -1.8663e-01,\n",
      "          -1.9775e-02,  4.0906e-01, -2.2585e-01, -2.7884e-01,  3.0589e-01,\n",
      "          -1.8086e-01, -1.1955e-01,  6.0635e-02, -9.8700e-02, -5.2228e-01,\n",
      "           4.2692e-01,  2.9542e-01,  6.8080e-02, -3.0555e-01,  2.2381e-01,\n",
      "          -1.1057e-01, -2.5209e-01, -3.7492e-01, -2.8756e-01],\n",
      "         [ 9.2668e-02,  9.8187e-02,  2.4271e-02,  2.2840e-01,  3.6014e-02,\n",
      "           3.0891e-01, -6.8656e-02,  4.2153e-02, -3.7581e-01,  4.7698e-02,\n",
      "          -1.1771e-01, -1.8148e-02,  1.9124e-01,  2.4814e-01,  2.5443e-01,\n",
      "          -2.2479e-01,  3.5336e-01,  2.6577e-01,  7.4504e-02,  2.3937e-01,\n",
      "           1.1321e-01, -8.3870e-02,  4.0645e-02, -3.1911e-01,  3.0254e-01,\n",
      "          -1.5439e-01,  4.9571e-02, -2.8933e-01, -1.2138e-01,  1.7146e-01,\n",
      "           1.5906e-01, -5.8591e-01,  4.3238e-01, -1.5215e-01,  2.2671e-01,\n",
      "           3.8341e-02,  2.2714e-01,  1.4781e-02,  1.7823e-02, -3.2098e-01,\n",
      "           6.5048e-02,  2.2834e-01,  5.3740e-01, -2.1524e-01, -2.8118e-01,\n",
      "          -2.5298e-02,  3.5300e-01, -1.3544e-01, -2.3657e-01,  2.4893e-01,\n",
      "          -1.0683e-01, -1.3585e-01, -3.9041e-02, -9.0829e-02, -4.8196e-01,\n",
      "           3.0583e-01,  2.3394e-01,  6.2786e-02, -2.0574e-01,  2.1939e-01,\n",
      "          -4.2025e-02, -2.4269e-01, -3.4718e-01, -2.9375e-01]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "\n",
      "Test passed ✅\n"
     ]
    }
   ],
   "source": [
    "def test_self_attention():\n",
    "    import torch\n",
    "    from torch import nn\n",
    "\n",
    "    batch_size = 2\n",
    "    seq_len = 6\n",
    "    d_embed = 64\n",
    "    n_heads = 4\n",
    "\n",
    "    # Dummy input\n",
    "    x = torch.randn(batch_size, seq_len, d_embed)\n",
    "\n",
    "    # Simulated padding mask: 1 = valid, 0 = pad\n",
    "    # Second sequence is shorter and padded at the end\n",
    "    attention_mask = torch.tensor([\n",
    "        [1, 1, 1, 1, 1, 1],    # No padding\n",
    "        [1, 1, 1, 0, 0, 0]     # Last 3 are padding\n",
    "    ])\n",
    "\n",
    "    # Instantiate the SelfAttention module\n",
    "    self_attn = SelfAttention(n_heads=n_heads, d_embed=d_embed)\n",
    "\n",
    "    # Test without causal mask\n",
    "    print(\"=== Without Causal Mask ===\")\n",
    "    out = self_attn(x, attention_mask=attention_mask, causal_mask=False)\n",
    "    print(\"Output shape:\", out.shape)\n",
    "\n",
    "    # Test with causal mask\n",
    "    print(\"\\n=== With Causal Mask ===\")\n",
    "    out_causal = self_attn(x, attention_mask=attention_mask, causal_mask=True)\n",
    "    print(\"Output shape:\", out_causal)\n",
    "\n",
    "    # Verify no NaNs\n",
    "    assert not torch.isnan(out).any(), \"Output has NaNs\"\n",
    "    assert not torch.isnan(out_causal).any(), \"Causal output has NaNs\"\n",
    "    print(\"\\nTest passed ✅\")\n",
    "\n",
    "\n",
    "test_self_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetImageEncoder(nn.Module):\n",
    "    def __init__(self,n_heads,d_cross=2048, d_model=512):\n",
    "        super(ResnetImageEncoder, self).__init__()\n",
    "        \n",
    "        # ResNet backbone\n",
    "        resnet = models.resnet101(weights='ResNet101_Weights.IMAGENET1K_V2')\n",
    "        self.model = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        \n",
    "        #checkpointing for low memory\n",
    "        #self.segements=segments\n",
    "        # Self-attention and normalization\n",
    "        self.attn_self = SelfAttention(n_heads, d_cross)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_cross) * 0.02,requires_grad=True)  # Intializing with std=0.02 as mentioned in the Vit paper\n",
    "        self.layernorm = LayerNormalization(d_cross)\n",
    "        self.dropout=nn.Dropout(0.3)\n",
    "        \n",
    "        # Projection for cross-attention\n",
    "        self.proj_cross = nn.Linear(d_cross, d_model)\n",
    "        \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x=self.model(x)\n",
    "        #x=checkpoint_sequential(self.model, segments=10, input=x)\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        x = x.permute(0, 2, 3, 1).view(batch_size, height*width, channels)\n",
    "        \n",
    "        #necessary since we will get siglip loss for this \n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        \n",
    "       #Pre-ln with dropout recommend in many papers\n",
    "        attn_output = self.layernorm(x)\n",
    "        x= x + self.dropout(self.attn_self(attn_output,causal_mask=False))\n",
    "        \n",
    "       \n",
    "        # cls_representation = x[:, 0]  # [b, 2048]\n",
    "        # contrastive_output = self.proj_contrastive(cls_representation)  # [b, d_model]#<-There is no need to use this since its better to use [b,seq_len,d_model]\n",
    "        \n",
    "        # Create sequence representation for cross-attention\n",
    "        cross_attn_output = self.proj_cross(x)  # [b, seq_len, d_model]\n",
    "        \n",
    "        return cross_attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, d_model: int, eps=1e-5): #Standard value for eps\n",
    "        super().__init__() #Corrected super init\n",
    "        self.para1 = nn.Parameter(torch.ones(d_model)) \n",
    "        self.para2 = nn.Parameter(torch.zeros(d_model)) \n",
    "        self.eps = eps\n",
    "    #updated this to support mixed precision training\n",
    "    def forward(self, x):\n",
    "        #Imp had forgotten earlier\n",
    "        if not self.training:\n",
    "         return x\n",
    "        x_float=x.to(torch.float32)\n",
    "        mean = x_float.mean(dim=-1, keepdim=True) \n",
    "        std_normal = x_float.std(dim=-1,unbiased=False, keepdim=True) \n",
    "        normalized = (x - mean.to(x.dtype)) / torch.sqrt(std_normal + self.eps).to(x.dtype)\n",
    "        return (self.para1 * normalized) + self.para2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if __name__ == \\'__main__\\':\\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\n\\n    batch_size = 4\\n    seq_len = 10\\n    d_model = 32\\n    num_experts = 8\\n    ffn_dim = d_model * 4 # Example FFN dim\\n\\n    router_layer = DeepSeekRouter(\\n        d_model=d_model,\\n        num_experts=num_experts,\\n        d_ff=ffn_dim,\\n        capacity_factor=1.25, # Example capacity factor\\n        loss_coef=0.01        # Example loss coefficient\\n    ).to(device)\\n\\n    input_tensor = torch.randn(batch_size, seq_len, d_model, device=device)\\n\\n    router_layer.train()\\n    output_tensor, aux_loss = router_layer(input_tensor)\\n\\n    print(\"Input Shape:\", input_tensor.shape)\\n    print(\"Output Shape:\", output_tensor)\\n    print(f\"Auxiliary Loss: {aux_loss.item():.4f}\")\\n\\n\\n    router_layer.eval()\\n    with torch.no_grad():\\n         output_inf, _ = router_layer(input_tensor)\\n    print(\"Inference output shape:\", output_inf.shape)'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math # Added for ceil could also use +1 whatever\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "\n",
    "class DeepSeekRouter(nn.Module):\n",
    "    def __init__(self,d_ff,d_model, num_experts, capacity_factor=1.0,loss_coef=1e-2): \n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.hidden_size = d_model\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.ffn_dim = d_ff\n",
    "        self.loss_coef = loss_coef \n",
    "\n",
    "        self.router = nn.Linear(d_model, num_experts) \n",
    "        # Create expert FFNs\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(d_model, d_ff),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(d_ff, d_model)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "        # Initialize weights \n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        # Initialize router\n",
    "        #all the values have been taken from the switch transformer paper plus recommended by chatgpt so yeah\n",
    "        nn.init.xavier_uniform_(self.router.weight)\n",
    "        if self.router.bias is not None:\n",
    "            nn.init.zeros_(self.router.bias)\n",
    "\n",
    "        for expert in self.experts:\n",
    "            for name, param in expert.named_parameters():\n",
    "                if param.dim() > 1:\n",
    "                    # Kaiming for Linear layers before GELU, Xavier for the last Linear\n",
    "                    if '0.weight' in name: # First linear layer\n",
    "                         nn.init.kaiming_uniform_(param, mode='fan_in', nonlinearity='relu') # GELU ~ ReLU for init will have to research more since we are using gelu\n",
    "                    elif '2.weight' in name: # Second linear layer\n",
    "                         nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    nn.init.zeros_(param)\n",
    "\n",
    "    def calculate_capacity(self, num_tokens):\n",
    "        capacity = math.ceil(num_tokens * self.capacity_factor / self.num_experts)\n",
    "        capacity = max(1, int(capacity))\n",
    "        # ensure at min it has capacity that doesn't exceed total tokens\n",
    "        capacity = min(capacity, num_tokens)\n",
    "        return capacity\n",
    "\n",
    "    def process_expert_batch(self, expert_inputs):\n",
    "        \"\"\" Process batched inputs through each expert using activation checkpointing. \"\"\"\n",
    "\n",
    "        #(Expert_index,cap,d_model)<-shape\n",
    "        expert_outputs = torch.zeros_like(expert_inputs)\n",
    "\n",
    "        for expert_idx, expert in enumerate(self.experts):\n",
    "            # Get the specific inputs for this expert\n",
    "            current_expert_inputs = expert_inputs[expert_idx] # Shape: (capacity, hidden_size)\n",
    "\n",
    "            # Identify which tokens within this expert's batch are actual tokens (not padding)\n",
    "            valid_token_mask_expert = current_expert_inputs.abs().sum(dim=-1) > 1e-6\n",
    "\n",
    "            if valid_token_mask_expert.any():\n",
    "                 # Select only the valid tokens for processing\n",
    "\n",
    "                valid_inputs = current_expert_inputs[valid_token_mask_expert]\n",
    "                 #This is using deepspeed checkpointng could also use pytorchs but the training is done in deepspeed\n",
    "                 # Apply checkpointing: pass the expert module and its valid inputs\n",
    "                 # checkpoint will handle calling expert(valid_inputs) internally\n",
    "                 #Lol had to use it\n",
    "                valid_tokens=checkpoint(expert,valid_inputs,use_reentrant=False)\n",
    "                 # Place the results back into the correct positions in the output buffer\n",
    "                expert_outputs[expert_idx, valid_token_mask_expert]=valid_tokens\n",
    "                 #expert_outputs[expert_idx, valid_token_mask_expert] =   expert(expert_inputs[expert_idx, valid_token_mask_expert])\n",
    "            # else: No valid tokens for this expert, expert_outputs remains zeros\n",
    "\n",
    "        return expert_outputs\n",
    "\n",
    "    def calculate_load_balancing_loss(self, router_probs, final_dispatch_mask_bool):\n",
    "        \"\"\" Calculate aux loss based on router probs and final dispatch counts \"\"\"\n",
    "        num_tokens, num_experts = router_probs.shape\n",
    "        if num_tokens == 0:\n",
    "            return torch.tensor(0.0, device=router_probs.device)\n",
    "\n",
    "        tokens_per_expert = final_dispatch_mask_bool.float().sum(dim=0)\n",
    "        fraction_tokens_per_expert = tokens_per_expert / num_tokens # f_i\n",
    "\n",
    "        mean_router_prob_per_expert = router_probs.mean(dim=0) # P_i\n",
    "\n",
    "        loss = self.num_experts * torch.sum(fraction_tokens_per_expert * mean_router_prob_per_expert)\n",
    "        return loss * self.loss_coef\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        num_tokens = batch_size * seq_len\n",
    "\n",
    "        if num_tokens == 0:\n",
    "            return x, torch.tensor(0.0, device=x.device)\n",
    "\n",
    "        # Flatten the input for routing\n",
    "        flatten_x = x.view(-1, d_model) # Shape: (num_tokens, d_model)\n",
    "\n",
    "        # 1. Get router probabilities\n",
    "        router_logits = self.router(flatten_x)\n",
    "        router_probs = F.softmax(router_logits, dim=-1, dtype=torch.float32) # Use float32 for mixed precision\n",
    "\n",
    "        expert_weights_topk, expert_indices_topk = torch.topk(router_probs, k=2, dim=-1)\n",
    "\n",
    "        # Normalize weights across top-k (important if combining both, good practice anyway)#from chatgpt\n",
    "        # Using float32 for stability\n",
    "        normalized_expert_weights_topk = F.softmax(expert_weights_topk, dim=-1, dtype=torch.float32)\n",
    "\n",
    "        # Extract primary and secondary assignments (use normalized weights)\n",
    "        primary_expert_indices = expert_indices_topk[:, 0]\n",
    "        primary_expert_weights = normalized_expert_weights_topk[:, 0] # Shape: (num_tokens,)\n",
    "        secondary_expert_indices = expert_indices_topk[:, 1]\n",
    "        secondary_expert_weights = normalized_expert_weights_topk[:, 1] # Shape: (num_tokens,)\n",
    "\n",
    "        # 3. Calculate Capacity\n",
    "        capacity = self.calculate_capacity(num_tokens)\n",
    "\n",
    "        # 4. Initial Assignment & Capacity Check (Primary Only First)\n",
    "        token_counts = torch.bincount(primary_expert_indices, minlength=self.num_experts)\n",
    "        over_capacity = token_counts > capacity\n",
    "\n",
    "        # 5. Handle Overflow: Reroute to Secondary (Simplified Approach)\n",
    "        # IMPORTANT LIMITATION: Does not check secondary expert capacity after rerouting.\n",
    "        final_expert_indices = primary_expert_indices.clone()\n",
    "        final_expert_weights = primary_expert_weights.clone()\n",
    "\n",
    "\n",
    "        for expert_idx in torch.where(over_capacity)[0]:\n",
    "            expert_mask = (primary_expert_indices == expert_idx)\n",
    "            expert_token_indices = torch.where(expert_mask)[0]\n",
    "\n",
    "            # Find indices of tokens assigned to this primary expert that exceed capacity\n",
    "            overflow_indices = expert_token_indices[capacity:] \n",
    "\n",
    "            # Reroute these specific tokens to their secondary choice\n",
    "            final_expert_indices[overflow_indices] = secondary_expert_indices[overflow_indices]\n",
    "            final_expert_weights[overflow_indices] = secondary_expert_weights[overflow_indices]\n",
    "\n",
    "        # 6. Determine Final Dispatch Positions (respecting capacity post-rerouting for *each expert*)\n",
    "        # Re-calculate counts based on final assignments\n",
    "        #final_token_counts = torch.bincount(final_expert_indices, minlength=self.num_experts)\n",
    "\n",
    "        # Create final dispatch mask and positions\n",
    "        final_dispatch_mask_bool = torch.zeros((num_tokens, self.num_experts), dtype=torch.bool, device=x.device)\n",
    "        dispatch_positions = torch.zeros_like(final_expert_indices)\n",
    "\n",
    "        # Use cumsum approach on final assignments to get positions within capacity limits\n",
    "        current_expert_counts = torch.zeros(self.num_experts, dtype=torch.long, device=x.device)\n",
    "        for token_idx in range(num_tokens):\n",
    "            expert_id = final_expert_indices[token_idx].item()\n",
    "            current_count = current_expert_counts[expert_id]\n",
    "            if current_count < capacity:\n",
    "                 final_dispatch_mask_bool[token_idx, expert_id] = True\n",
    "                 dispatch_positions[token_idx] = current_count\n",
    "                 current_expert_counts[expert_id] += 1\n",
    "            # Else: Token is dropped if its final assigned expert is full\n",
    "\n",
    "        # Calculate Aux Loss based on original probs and final dispatch mask\n",
    "        aux_loss = self.calculate_load_balancing_loss(router_probs, final_dispatch_mask_bool)\n",
    "\n",
    "        # 7. Gather Inputs for Experts\n",
    "        # Get indices of tokens that are actually dispatched\n",
    "        #dispatch_token_indices = torch.where(final_dispatch_mask_bool.sum(dim=1) > 0)[0]\n",
    "        dispatch_token_indices, assigned_expert_indices = torch.nonzero(final_dispatch_mask_bool, as_tuple=True)\n",
    "\n",
    "        if dispatch_token_indices.numel() == 0:\n",
    "             # Handle case where no tokens are dispatched\n",
    "             return torch.zeros_like(x), aux_loss\n",
    "\n",
    "        # Get assignments for dispatched tokens\n",
    "        assigned_expert = assigned_expert_indices#<-Tells which we which expert was selected for per token\n",
    "        assigned_position = dispatch_positions[dispatch_token_indices]#<-Imp remember this gives us the capacity\n",
    "\n",
    "        # Allocate expert storage\n",
    "        expert_inputs = torch.zeros(self.num_experts, capacity, d_model, dtype=x.dtype, device=x.device)\n",
    "\n",
    "        # Scatter inputs using direct indexing (safer)\n",
    "        expert_inputs[assigned_expert, assigned_position] = flatten_x[dispatch_token_indices]\n",
    "\n",
    "        # 8. Process tokens with experts\n",
    "        expert_outputs_buffer = self.process_expert_batch(expert_inputs)\n",
    "\n",
    "        # 9. Gather results back & Apply Weights\n",
    "        output_flat = torch.zeros_like(flatten_x)\n",
    "\n",
    "        # Retrieve outputs for dispatched tokens\n",
    "        dispatched_outputs = expert_outputs_buffer[assigned_expert, assigned_position] # Shape: (num_dispatched, d_model)\n",
    "\n",
    "        # Get weights for dispatched tokens\n",
    "        dispatched_weights = final_expert_weights[dispatch_token_indices] # Shape: (num_dispatched,)\n",
    "\n",
    "        # Weight the outputs\n",
    "        weighted_outputs = dispatched_outputs * dispatched_weights.unsqueeze(-1).to(x.dtype)\n",
    "\n",
    "        # Scatter back using index_add_\n",
    "        output_flat.index_add_(0, dispatch_token_indices, weighted_outputs)\n",
    "\n",
    "        # 10. Reshape back to original dimensions\n",
    "        output = output_flat.view(batch_size, seq_len, d_model)\n",
    "\n",
    "        return output, aux_loss\n",
    "\n",
    "# Example Usage (similar to before)\n",
    "\"\"\"if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    batch_size = 4\n",
    "    seq_len = 10\n",
    "    d_model = 32\n",
    "    num_experts = 8\n",
    "    ffn_dim = d_model * 4 # Example FFN dim\n",
    "\n",
    "    router_layer = DeepSeekRouter(\n",
    "        d_model=d_model,\n",
    "        num_experts=num_experts,\n",
    "        d_ff=ffn_dim,\n",
    "        capacity_factor=1.25, # Example capacity factor\n",
    "        loss_coef=0.01        # Example loss coefficient\n",
    "    ).to(device)\n",
    "\n",
    "    input_tensor = torch.randn(batch_size, seq_len, d_model, device=device)\n",
    "\n",
    "    router_layer.train()\n",
    "    output_tensor, aux_loss = router_layer(input_tensor)\n",
    "\n",
    "    print(\"Input Shape:\", input_tensor.shape)\n",
    "    print(\"Output Shape:\", output_tensor)\n",
    "    print(f\"Auxiliary Loss: {aux_loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    router_layer.eval()\n",
    "    with torch.no_grad():\n",
    "         output_inf, _ = router_layer(input_tensor)\n",
    "    print(\"Inference output shape:\", output_inf.shape)\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, d_cross, n_heads, d_ff, num_experts, \n",
    "                 capacity_factor=1.0,dropout=0.1, strategy=\"flamingo\"):\n",
    "        super().__init__()\n",
    "        self.strategy = strategy.lower()\n",
    "        \n",
    "        # Self-Attention\n",
    "        self.norm1 = LayerNormalization(d_model)\n",
    "        self.self_attn = SelfAttention(n_heads, d_model)\n",
    "        \n",
    "        # Cross-Attention\n",
    "        self.norm2 = LayerNormalization(d_model)\n",
    "        self.norm_encoder = LayerNormalization(d_cross)\n",
    "        self.cross_attn = CrossAttention(n_heads, d_model, d_cross)\n",
    "        \n",
    "        # Reverse Cross-Attention\n",
    "        self.norm_img = LayerNormalization(d_cross)\n",
    "        self.norm_rev_cross = LayerNormalization(d_model)\n",
    "        self.rev_cross_attn = CrossAttention(n_heads, d_cross, d_model)\n",
    "        \n",
    "        # Gated Fusion (if enabled)\n",
    "        if self.strategy == \"gating\":\n",
    "            self.fusion_gate = nn.Sequential(\n",
    "                nn.Linear(d_model * 2, d_model),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "            self.fusion_norm = LayerNormalization(d_model)\n",
    "        \n",
    "        # MoE Feed-Forward\n",
    "        self.norm3 = LayerNormalization(d_model)\n",
    "        self.moe = DeepSeekRouter(d_ff=d_ff,d_model=d_model, num_experts=num_experts, capacity_factor=capacity_factor,loss_coef=1e-2)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, attention_mask=None, encoder_output=None,):\n",
    "        aux_loss_total = 0.0\n",
    "        \n",
    "        x_norm = self.norm1(x)\n",
    "        self_attn_out = self.self_attn(x_norm, attention_mask=attention_mask, causal_mask=True)\n",
    "        x = x + self.dropout(self_attn_out)\n",
    "        \n",
    "        # Only proceed with cross-attention if encoder output is provided\n",
    "        if encoder_output is not None:\n",
    "            # Cross-Attention Block (Pre-LN)\n",
    "            x_norm = self.norm2(x)\n",
    "            encoder_norm = self.norm_encoder(encoder_output)\n",
    "            cross_attn_out = self.cross_attn(x_norm, encoder_norm)\n",
    "            enhanced_x = x + self.dropout(cross_attn_out)\n",
    "            \n",
    "            # Reverse Cross-Attention (Pre-LN)\n",
    "            enhanced_x_norm = self.norm_rev_cross(enhanced_x)\n",
    "            encoder_norm_img = self.norm_img(encoder_output)\n",
    "            rev_cross_out = self.rev_cross_attn(encoder_norm_img, enhanced_x_norm)\n",
    "            enhanced_encoder = encoder_output + self.dropout(rev_cross_out)\n",
    "            \n",
    "            # Apply strategy based on the various papers\n",
    "            \n",
    "            if self.strategy == \"vilbert\":\n",
    "                #print(f\"Using : {self.strategy}\")\n",
    "                x = enhanced_encoder\n",
    "            elif self.strategy == \"gating\":\n",
    "               # print(f\"Using strategy: {self.strategy}\")\n",
    "                fusion_input = torch.cat((enhanced_x, enhanced_encoder), dim=-1)\n",
    "                gate = self.fusion_gate(fusion_input)\n",
    "                x = enhanced_x * (1 - gate) + enhanced_encoder * gate\n",
    "                x = self.fusion_norm(x)\n",
    "            elif self.strategy == \"flamingo\":\n",
    "                #print(f\"Using strategy: {self.strategy}\")\n",
    "                x = enhanced_x\n",
    "            else:\n",
    "                #print(f\"Since no stratergy is selected default case for : {self.strategy}\")\n",
    "                x = enhanced_encoder + enhanced_x\n",
    "        \n",
    "        # MoE Feed-Forward Block (Pre-LN)\n",
    "        x_norm = self.norm3(x)\n",
    "        moe_out, moe_aux_loss = self.moe(x_norm)\n",
    "        x = x + self.dropout(moe_out)\n",
    "        aux_loss_total += moe_aux_loss\n",
    "                        \n",
    "        #print(f\"x shape: {x.shape}\")\n",
    "        #print(f\"enhanced_x shape: {enhanced_x.shape}\")\n",
    "        #print(f\"enhanced_encoder shape: {enhanced_encoder.shape}\")\n",
    "        \n",
    "        return x, enhanced_x, enhanced_encoder, aux_loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand(3,50,512)\n",
    "z=torch.rand(3,50,512)\n",
    "a=torch.randint(0,1,(1,50))\n",
    "y=DecoderBlock(512,512,6,1024,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "DecoderBlock.forward() got an unexpected keyword argument 'attention_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 42\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(aux_loss, torch\u001b[39m.\u001b[39mTensor), \u001b[39m\"\u001b[39m\u001b[39mAux loss not returned as tensor\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m✅ DecoderBlock test passed!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 42\u001b[0m test_decoder_block()\n",
      "Cell \u001b[0;32mIn[86], line 28\u001b[0m, in \u001b[0;36mtest_decoder_block\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m decoder \u001b[39m=\u001b[39m DecoderBlock(\n\u001b[1;32m     19\u001b[0m     d_model\u001b[39m=\u001b[39md_model,\n\u001b[1;32m     20\u001b[0m     d_cross\u001b[39m=\u001b[39md_cross,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     strategy\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mvilbert\u001b[39m\u001b[39m\"\u001b[39m  \u001b[39m# Try \"vilbert\", \"flamingo\", etc.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m x_out, enhanced_x, enhanced_encoder, aux_loss \u001b[39m=\u001b[39m decoder(\n\u001b[1;32m     29\u001b[0m     x,\n\u001b[1;32m     30\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m     31\u001b[0m     encoder_output\u001b[39m=\u001b[39;49mencoder_output\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[39m# Assertions\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39massert\u001b[39;00m x_out\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (batch_size, seq_len, d_model), \u001b[39m\"\u001b[39m\u001b[39mOutput shape mismatch\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "\u001b[0;31mTypeError\u001b[0m: DecoderBlock.forward() got an unexpected keyword argument 'attention_mask'"
     ]
    }
   ],
   "source": [
    "def test_decoder_block():\n",
    "    import torch\n",
    "    batch_size = 2\n",
    "    seq_len = 5\n",
    "    img_len = 3\n",
    "    d_model = 64\n",
    "    d_cross = 64\n",
    "    n_heads = 8\n",
    "    d_ff = 128\n",
    "    num_experts = 4\n",
    "\n",
    "    # Create dummy data\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    encoder_output = torch.randn(batch_size, img_len, d_cross)\n",
    "    attention_mask = torch.ones(batch_size, seq_len)  # no padding\n",
    "\n",
    "    # Instantiate decoder block\n",
    "    decoder = DecoderBlock(\n",
    "        d_model=d_model,\n",
    "        d_cross=d_cross,\n",
    "        n_heads=n_heads,\n",
    "        d_ff=d_ff,\n",
    "        num_experts=num_experts,\n",
    "        strategy=\"vilbert\"  # Try \"vilbert\", \"flamingo\", etc.\n",
    "    )\n",
    "\n",
    "    # Forward pass\n",
    "    x_out, enhanced_x, enhanced_encoder, aux_loss = decoder(\n",
    "        x,\n",
    "        attention_mask=attention_mask,\n",
    "        encoder_output=encoder_output\n",
    "    )\n",
    "\n",
    "    # Assertions\n",
    "    assert x_out.shape == (batch_size, seq_len, d_model), \"Output shape mismatch\"\n",
    "    assert enhanced_x.shape == (batch_size, seq_len, d_model), \"Enhanced_x shape mismatch\"\n",
    "    assert enhanced_encoder.shape == (batch_size, img_len, d_cross), \"Enhanced encoder shape mismatch\"\n",
    "    assert isinstance(aux_loss, torch.Tensor), \"Aux loss not returned as tensor\"\n",
    "\n",
    "    print(\"✅ DecoderBlock test passed!\")\n",
    "\n",
    "test_decoder_block()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlocks(nn.Module):\n",
    "    def __init__(self, num_layers, d_model, d_cross, n_heads, d_ff, num_experts, \n",
    "                 capacity_factor=1.0, dropout=0.1, strategy=\"flamingo\"):\n",
    "        super().__init__()\n",
    "        self.strategy = strategy.lower()\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(d_model, d_cross, n_heads, d_ff, num_experts, capacity_factor, dropout, strategy)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = LayerNormalization(d_model)\n",
    "    \n",
    "    def forward(self, x, encoder_output=None):\n",
    "        aux_loss_total = 0.0\n",
    "        if encoder_output is not None:\n",
    "            if self.strategy == \"gating\" or encoder_output.size(1) > x.size(1):\n",
    "                encoder_output = encoder_output[:, :x.size(1), :]\n",
    "            for layer in self.layers:\n",
    "                x, enhanced_x, enhanced_encoder, aux_loss = layer(x, encoder_output)\n",
    "                aux_loss_total += aux_loss\n",
    "                encoder_output = enhanced_encoder\n",
    "                #encoder_output=enhanced_x\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        return x, aux_loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import CLIPTextModel\n",
    "\n",
    "class ClipTextEncoder(nn.Module):\n",
    "    def __init__(self, dropout=0.1, model_name=\"openai/clip-vit-base-patch32\", freeze_clip=False):\n",
    "        super().__init__()\n",
    "        self.clip_text = CLIPTextModel.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "        if freeze_clip:\n",
    "            for param in self.clip_text.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.clip_text(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return self.dropout(outputs.last_hidden_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, num_layers: int, d_model: int, d_cross: int, n_heads: int, d_ff: int, \n",
    "                 num_experts: int, vocab_size: int, strategy:str):\n",
    "        super().__init__()\n",
    "        self.encoder = ResnetImageEncoder(n_heads)\n",
    "        self.decoder = DecoderBlocks(num_layers, d_model, d_cross, n_heads, d_ff, num_experts,capacity_factor=1.0, dropout=0.1, strategy=strategy)\n",
    "        self.fc1 = nn.Linear(d_model, vocab_size)  # This is the output layer\n",
    "        self.embedding = ClipTextEncoder()\n",
    "        for param in self.embedding.parameters():\n",
    "         param.data = param.data.to(torch.bfloat16)\n",
    "        self.img_embedding = nn.Linear(512, d_model)\n",
    "        self.text_embedding = nn.Linear(d_model, d_model)\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * torch.log(torch.tensor(1 / 0.07)))\n",
    "        self.d_model=d_model\n",
    "\n",
    "    def get_img_embedding(self, image):\n",
    "        img_emb = self.encoder(image)\n",
    "        img_emb = img_emb[:, 0]  #<- Extract [CLS] token representation using for the resnet atm\n",
    "        return self.img_embedding(img_emb)\n",
    "    \n",
    "    def get_text_embedding(self, captions, encoder_output,attention_mask=None):\n",
    "        captions_embeds = self.embedding(captions,attention_mask=attention_mask)\n",
    "        x, _ = self.decoder(captions_embeds, encoder_output=encoder_output)\n",
    "        x = x[:, 0]  # Assuming CLS token is the representation\n",
    "        return self.text_embedding(x)\n",
    "    \n",
    "    def siglip_constrastive_loss(self, text_emb, img_emb):\n",
    "        batch_size = img_emb.shape[0]\n",
    "        img_emb = F.normalize(img_emb, dim=1)  # Normalize for cosine similarity\n",
    "        text_emb = F.normalize(text_emb, dim=1)  # Normalize for cosine similarity\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logit_per_image = logit_scale * img_emb @ text_emb.t()\n",
    "        logit_per_text = logit_per_image.t()\n",
    "        labels = torch.eye(batch_size, device=img_emb.device)\n",
    "        img_loss = F.binary_cross_entropy_with_logits(logit_per_image, labels)#<-since we need similar score thats why\n",
    "        text_loss = F.binary_cross_entropy_with_logits(logit_per_text, labels)\n",
    "        return (img_loss + text_loss) / 2\n",
    "    \n",
    "    def forward(self, images, captions,attention_mask=None,teacher_forcing_ratio=1.0):\n",
    "        device = images.device\n",
    "        batch_size, seq_len = captions.shape\n",
    "        \n",
    "        # Get image embeddings\n",
    "        encoder_output = self.encoder(images)\n",
    "        image_embed = self.get_img_embedding(images)\n",
    "        \n",
    "        # Prepare decoder inputs (remove last token from captions)\n",
    "        decoder_input = captions[:, :-1]\n",
    "        captions_embeds = self.embedding(decoder_input,attention_mask=attention_mask)\n",
    "        \n",
    "        # Full teacher forcing or evaluation mode\n",
    "        if not self.training or teacher_forcing_ratio == 1.0:\n",
    "            # Forward pass through decoder\n",
    "            decoder_output, aux_loss = self.decoder(captions_embeds, encoder_output=encoder_output)\n",
    "            \n",
    "            # Apply output projection\n",
    "            logits = self.fc1(decoder_output)\n",
    "            \n",
    "            # Get text embeddings for contrastive loss\n",
    "            text_embeds = self.get_text_embedding(captions, encoder_output=encoder_output)\n",
    "            clip_loss = self.siglip_constrastive_loss(text_embeds, image_embed)\n",
    "            #predicted_tokens = logits.argmax(dim=-1)  # shape: (batch_size, seq_len-1)\n",
    "            #targets = captions[:, 1:]  # ground truth tokens from position 1 onwards\n",
    "\n",
    "            # For a sample in the batch, convert indices to tokens \n",
    "            #print(\"Predicted tokens for sample 0:\", predicted_tokens[0])\n",
    "            #print(\"Target tokens for sample 0:   \", targets[0])\n",
    "            \n",
    "            return logits, aux_loss, clip_loss\n",
    "        \n",
    "        # Partial teacher forcing (during training)<-Logic is flawed dont use this \n",
    "        decoder_output = torch.zeros(batch_size, seq_len-1, 512, device=device)\n",
    "        aux_losses = []\n",
    "        \n",
    "        # Start with first token\n",
    "        current_input = captions_embeds[:, 0:1]\n",
    "        assert captions[:, :-1].shape == captions[:, 1:].shape, \"Shapes of decoder inputs and targets do not match!\"\n",
    "        \n",
    "        for t in range(seq_len - 1):\n",
    "            # For subsequent tokens, concatenate with previous tokens\n",
    "            if t == 0:\n",
    "                step_output, step_aux_loss = self.decoder(\n",
    "                    current_input,\n",
    "                    encoder_output=encoder_output)\n",
    "            else:\n",
    "                accumulated_input = torch.cat([captions_embeds[:, :t], current_input], dim=1)\n",
    "                step_output, step_aux_loss = self.decoder(\n",
    "                    accumulated_input,\n",
    "                    encoder_output=encoder_output)\n",
    "            \n",
    "            aux_losses.append(step_aux_loss)\n",
    "            decoder_output[:, t:t+1] = step_output[:, -1:]\n",
    "            \n",
    "            # Decide whether to use teacher forcing for next token\n",
    "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            # Don't need to compute next input for the last iteration\n",
    "            #Earlier it had t<seq_len-2\n",
    "            if use_teacher_forcing:\n",
    "                    next_input = captions_embeds[:, t+1:t+2]\n",
    "            else:\n",
    "                # ...\n",
    "                top1_pred = step_output.argmax(2)\n",
    "\n",
    "                if top1_pred.dtype != captions.dtype:\n",
    "                 top1_pred = top1_pred.to(captions.dtype)\n",
    "\n",
    "                # This self.embedding is the DECODER's embedding layer\n",
    "                next_input_embedding = self.embedding(top1_pred)\n",
    "\n",
    "                # FIX: Cast if necessary\n",
    "                is_bf16_mode = step_output.dtype == torch.bfloat16\n",
    "\n",
    "                if is_bf16_mode and next_input_embedding.dtype == torch.float32:\n",
    "                    print('yes')\n",
    "                    next_input = next_input_embedding.to(torch.bfloat16)\n",
    "                else:\n",
    "                    next_input = next_input_embedding\n",
    "                \n",
    "                next_input=next_input.detach()\n",
    "                # ...\n",
    "            current_input = next_input\n",
    "        \n",
    "        # Apply output projection\n",
    "        logits = self.fc1(decoder_output)\n",
    "        #predicted_tokens = logits.argmax(dim=-1)  # shape: (batch_size, seq_len-1)\n",
    "        #targets = captions[:, 1:]  # ground truth tokens from position 1 onwards\n",
    "\n",
    "        # For a sample in the batch, convert indices to tokens (assuming you have a tokenizer or vocab mapping)\n",
    "        #print(\"Predicted tokens for sample 0:\", predicted_tokens[0])\n",
    "        #print(\"Target tokens for sample 0:   \", targets[0])\n",
    "        \n",
    "        # Get text embeddings from predictions\n",
    "        pred_tokens = logits.argmax(2)\n",
    "        final_sequence_embeds = self.get_text_embedding(pred_tokens, encoder_output=encoder_output,attention_mask=None)\n",
    "        \n",
    "        # Compute contrastive loss\n",
    "        clip_loss = self.siglip_constrastive_loss(final_sequence_embeds, image_embed)\n",
    "        combined_aux_loss = sum(aux_losses) / len(aux_losses) if aux_losses else 0\n",
    "        \n",
    "        return logits, combined_aux_loss, clip_loss\n",
    "    \n",
    "    def greedy_caption_generation(self,image,attention_mask=None, seq_len=50, start_idx_token=49406, end_idx_token=49407):\n",
    "        self.eval()\n",
    "        device = image.device\n",
    "        with torch.no_grad():\n",
    "            # Get image features\n",
    "            encoder_output = self.encoder(image.unsqueeze(0))\n",
    "            # Start with start token\n",
    "            current_token = torch.tensor([[start_idx_token]], device=device)\n",
    "            output_tokens = [start_idx_token]\n",
    "            \n",
    "            for _ in range(seq_len):\n",
    "                # Embed current sequence\n",
    "                token_emb = self.embedding(current_token,attention_mask=attention_mask)\n",
    "                \n",
    "                # Get decoder output\n",
    "                decoder_out, _ = self.decoder(token_emb, encoder_output=encoder_output)\n",
    "                \n",
    "                # Get logits and predicted token\n",
    "                logits = self.fc1(decoder_out[:, -1])\n",
    "                pred_token = logits.argmax(1)\n",
    "                \n",
    "                # Add to sequence\n",
    "                pred_token_item = pred_token.item()\n",
    "                output_tokens.append(pred_token_item)\n",
    "                current_token = torch.cat((current_token, pred_token.view(1,1)), dim=1)\n",
    "                \n",
    "                # Stop if end token\n",
    "                if pred_token_item == end_idx_token:\n",
    "                    break\n",
    "        \n",
    "        self.train()\n",
    "        return output_tokens[1:]  # Remove start token\n",
    "    \n",
    "    def beam_search_caption_generation(self,image,attention_mask=None, seq_len=50, start_idx_token=49406, end_idx_token=49407, beam_width=3):\n",
    "        self.eval()\n",
    "        device = image.device\n",
    "        with torch.no_grad():\n",
    "            # Get image features\n",
    "            encoder_output = self.encoder(image.unsqueeze(0))\n",
    "            \n",
    "            # Initialize beam\n",
    "            start_seq = torch.tensor([[start_idx_token]], device=device)\n",
    "            sequences = [(start_seq, 0.0, False)]  # (sequence, score, is_complete)            \n",
    "            # Beam search loop\n",
    "            for _ in range(seq_len):\n",
    "                # Break if all sequences are complete\n",
    "                if all(is_complete for _, _, is_complete in sequences):\n",
    "                    break\n",
    "                \n",
    "                candidates = []\n",
    "                \n",
    "                # Expand each sequence\n",
    "                for seq, score, is_complete in sequences:\n",
    "                    # Skip completed sequences\n",
    "                    if is_complete:\n",
    "                        candidates.append((seq, score, True))\n",
    "                        continue\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    token_emb = self.embedding(seq,attention_mask=attention_mask)\n",
    "                    decoder_out, _ = self.decoder(token_emb, encoder_output=encoder_output)\n",
    "                    logits = self.fc1(decoder_out[:, -1])\n",
    "                    \n",
    "                    # Get top-k tokens\n",
    "                    probs = F.softmax(logits, dim=-1)\n",
    "                    top_k_probs, top_k_indices = torch.topk(probs, beam_width)\n",
    "                    \n",
    "                    # Create new candidates\n",
    "                    for i in range(beam_width):\n",
    "                        next_token = top_k_indices[0, i]\n",
    "                        next_score = score - torch.log(top_k_probs[0, i]).item()  # Convert to log probability#check here if - or +\n",
    "                        #could also use score+top_k_probs[:,i] your choice\n",
    "                        next_seq = torch.cat((seq, next_token.view(1,1)), dim=1)\n",
    "                        is_end = (next_token == end_idx_token)\n",
    "                        \n",
    "                        candidates.append((next_seq, next_score, is_end))\n",
    "                \n",
    "                # Keep top sequences\n",
    "                sequences = sorted(candidates, key=lambda x: x[1])[:beam_width]\n",
    "            \n",
    "            # Return best completed sequence, or best incomplete if none completed\n",
    "            completed = [seq for seq, _, is_complete in sequences if is_complete]\n",
    "            if completed:\n",
    "                best_seq = completed[0]\n",
    "            else:\n",
    "                best_seq = sequences[0][0]\n",
    "            \n",
    "        self.train()\n",
    "        return best_seq.squeeze(0)[1:].tolist()  # Remove batch dim and start token\n",
    "    \n",
    "    def top_p_sampling(self,image,attention_mask=None, p=0.9, seq_len=50, temperature=1.0, start_idx_token=49406, end_idx_token=49407):\n",
    "        self.eval()\n",
    "        device = image.device\n",
    "        with torch.no_grad():\n",
    "            # Get image features\n",
    "            encoder_output = self.encoder(image.unsqueeze(0))\n",
    "            \n",
    "            # Start with start token\n",
    "            current_token = torch.tensor([[start_idx_token]], device=device)\n",
    "            output_tokens = [start_idx_token]\n",
    "\n",
    "            \n",
    "            for _ in range(seq_len):\n",
    "                # Embed current sequence\n",
    "                token_emb = self.embedding(current_token,attention_mask=attention_mask)\n",
    "                \n",
    "                # Get decoder output\n",
    "                decoder_out, _ = self.decoder(token_emb, encoder_output=encoder_output)\n",
    "                \n",
    "                # Get logits and apply temperature\n",
    "                logits = self.fc1(decoder_out[:, -1]) / temperature\n",
    "                \n",
    "                # Convert to probabilities\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                \n",
    "                # Sort probabilities\n",
    "                sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "                \n",
    "                # Compute cumulative probabilities\n",
    "                cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "                \n",
    "                # Find tokens within nucleus\n",
    "                nucleus = cumulative_probs < p\n",
    "                \n",
    "                # Ensure at least one token is selected\n",
    "                if not torch.any(nucleus):\n",
    "                    nucleus[0] = True\n",
    "                else:\n",
    "                    # Add first token that exceeds p\n",
    "                    nucleus_end = torch.argmax(cumulative_probs >= p)\n",
    "                    if nucleus_end > 0:  # Avoid adding again if it's the first token\n",
    "                        nucleus[nucleus_end] = True\n",
    "                \n",
    "                # Filter by nucleus\n",
    "                nucleus_probs = sorted_probs[nucleus]\n",
    "                nucleus_indices = sorted_indices[nucleus]\n",
    "                \n",
    "                # Sample token\n",
    "                sampled_idx = torch.multinomial(nucleus_probs, 1)\n",
    "                pred_token = nucleus_indices[sampled_idx].item()\n",
    "                \n",
    "                # Add to sequence\n",
    "                output_tokens.append(pred_token)\n",
    "                current_token = torch.cat((current_token, \n",
    "                                          torch.tensor([[pred_token]], device=device)), dim=1)\n",
    "                \n",
    "                # Stop if end token\n",
    "                if pred_token == end_idx_token:\n",
    "                    break\n",
    "        \n",
    "        self.train()\n",
    "        return output_tokens[1:]  # Remove start token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-05 13:35:31,741] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "Using /root/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /root/.cache/torch_extensions/py310_cu124/fused_adam/build.ninja...\n",
      "/root/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module fused_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load fused_adam op: 0.03848910331726074 seconds\n",
      "[2025-04-05 13:35:36,682] [INFO] [logging.py:107:log_dist] [Rank -1] DeepSpeed info: version=0.16.5, git-hash=unknown, git-branch=unknown\n",
      "[2025-04-05 13:35:36,683] [INFO] [comm.py:658:init_distributed] cdb=None\n",
      "[2025-04-05 13:35:36,684] [INFO] [comm.py:673:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2025-04-05 13:35:36,917] [INFO] [comm.py:728:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=172.17.0.2, master_port=29500\n",
      "[2025-04-05 13:35:36,919] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2025-04-05 13:35:36,924] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-05 13:35:37,330] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-04-05 13:35:37,333] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2025-04-05 13:35:37,333] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-04-05 13:35:37,426] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2025-04-05 13:35:37,427] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>\n",
      "[2025-04-05 13:35:37,427] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2025-04-05 13:35:37,428] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 500000000\n",
      "[2025-04-05 13:35:37,428] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 500000000\n",
      "[2025-04-05 13:35:37,429] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2025-04-05 13:35:37,429] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2025-04-05 13:35:38,122] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-04-05 13:35:38,126] [INFO] [utils.py:782:see_memory_usage] MA 1.1 GB         Max_MA 1.47 GB         CA 1.48 GB         Max_CA 1 GB \n",
      "[2025-04-05 13:35:38,127] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 9.22 GB, percent = 3.7%\n",
      "[2025-04-05 13:35:38,421] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-04-05 13:35:38,425] [INFO] [utils.py:782:see_memory_usage] MA 1.1 GB         Max_MA 1.83 GB         CA 2.21 GB         Max_CA 2 GB \n",
      "[2025-04-05 13:35:38,427] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 9.23 GB, percent = 3.7%\n",
      "[2025-04-05 13:35:38,428] [INFO] [stage_1_and_2.py:556:__init__] optimizer state initialized\n",
      "[2025-04-05 13:35:38,721] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-04-05 13:35:38,723] [INFO] [utils.py:782:see_memory_usage] MA 1.1 GB         Max_MA 1.1 GB         CA 2.21 GB         Max_CA 2 GB \n",
      "[2025-04-05 13:35:38,724] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 9.23 GB, percent = 3.7%\n",
      "[2025-04-05 13:35:38,739] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer\n",
      "[2025-04-05 13:35:38,740] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2025-04-05 13:35:38,740] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.SequentialLR object at 0x7f3080bca0e0>\n",
      "[2025-04-05 13:35:38,741] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-08], mom=[(0.9, 0.999)]\n",
      "[2025-04-05 13:35:38,743] [INFO] [config.py:1000:print] DeepSpeedEngine configuration:\n",
      "[2025-04-05 13:35:38,744] [INFO] [config.py:1004:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-04-05 13:35:38,744] [INFO] [config.py:1004:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-04-05 13:35:38,745] [INFO] [config.py:1004:print]   amp_enabled .................. False\n",
      "[2025-04-05 13:35:38,745] [INFO] [config.py:1004:print]   amp_params ................... False\n",
      "[2025-04-05 13:35:38,746] [INFO] [config.py:1004:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-04-05 13:35:38,747] [INFO] [config.py:1004:print]   bfloat16_enabled ............. True\n",
      "[2025-04-05 13:35:38,747] [INFO] [config.py:1004:print]   bfloat16_immediate_grad_update  True\n",
      "[2025-04-05 13:35:38,748] [INFO] [config.py:1004:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-04-05 13:35:38,748] [INFO] [config.py:1004:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-04-05 13:35:38,748] [INFO] [config.py:1004:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-04-05 13:35:38,749] [INFO] [config.py:1004:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f2e25492080>\n",
      "[2025-04-05 13:35:38,749] [INFO] [config.py:1004:print]   communication_data_type ...... None\n",
      "[2025-04-05 13:35:38,750] [INFO] [config.py:1004:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-04-05 13:35:38,750] [INFO] [config.py:1004:print]   curriculum_enabled_legacy .... False\n",
      "[2025-04-05 13:35:38,750] [INFO] [config.py:1004:print]   curriculum_params_legacy ..... False\n",
      "[2025-04-05 13:35:38,751] [INFO] [config.py:1004:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-04-05 13:35:38,751] [INFO] [config.py:1004:print]   data_efficiency_enabled ...... False\n",
      "[2025-04-05 13:35:38,752] [INFO] [config.py:1004:print]   dataloader_drop_last ......... False\n",
      "[2025-04-05 13:35:38,752] [INFO] [config.py:1004:print]   disable_allgather ............ False\n",
      "[2025-04-05 13:35:38,753] [INFO] [config.py:1004:print]   dump_state ................... False\n",
      "[2025-04-05 13:35:38,753] [INFO] [config.py:1004:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-04-05 13:35:38,754] [INFO] [config.py:1004:print]   eigenvalue_enabled ........... False\n",
      "[2025-04-05 13:35:38,754] [INFO] [config.py:1004:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-04-05 13:35:38,754] [INFO] [config.py:1004:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-04-05 13:35:38,755] [INFO] [config.py:1004:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-04-05 13:35:38,755] [INFO] [config.py:1004:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-04-05 13:35:38,756] [INFO] [config.py:1004:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-04-05 13:35:38,756] [INFO] [config.py:1004:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-04-05 13:35:38,756] [INFO] [config.py:1004:print]   eigenvalue_verbose ........... False\n",
      "[2025-04-05 13:35:38,757] [INFO] [config.py:1004:print]   elasticity_enabled ........... False\n",
      "[2025-04-05 13:35:38,757] [INFO] [config.py:1004:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 10, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 3, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-04-05 13:35:38,758] [INFO] [config.py:1004:print]   fp16_auto_cast ............... None\n",
      "[2025-04-05 13:35:38,758] [INFO] [config.py:1004:print]   fp16_enabled ................. False\n",
      "[2025-04-05 13:35:38,758] [INFO] [config.py:1004:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-04-05 13:35:38,759] [INFO] [config.py:1004:print]   global_rank .................. 0\n",
      "[2025-04-05 13:35:38,759] [INFO] [config.py:1004:print]   grad_accum_dtype ............. None\n",
      "[2025-04-05 13:35:38,760] [INFO] [config.py:1004:print]   gradient_accumulation_steps .. 4\n",
      "[2025-04-05 13:35:38,760] [INFO] [config.py:1004:print]   gradient_clipping ............ 1\n",
      "[2025-04-05 13:35:38,760] [INFO] [config.py:1004:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-04-05 13:35:38,761] [INFO] [config.py:1004:print]   graph_harvesting ............. False\n",
      "[2025-04-05 13:35:38,761] [INFO] [config.py:1004:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-04-05 13:35:38,762] [INFO] [config.py:1004:print]   initial_dynamic_scale ........ 1\n",
      "[2025-04-05 13:35:38,762] [INFO] [config.py:1004:print]   load_universal_checkpoint .... False\n",
      "[2025-04-05 13:35:38,762] [INFO] [config.py:1004:print]   loss_scale ................... 1.0\n",
      "[2025-04-05 13:35:38,763] [INFO] [config.py:1004:print]   memory_breakdown ............. False\n",
      "[2025-04-05 13:35:38,763] [INFO] [config.py:1004:print]   mics_hierarchial_params_gather  False\n",
      "[2025-04-05 13:35:38,764] [INFO] [config.py:1004:print]   mics_shard_size .............. -1\n",
      "[2025-04-05 13:35:38,764] [INFO] [config.py:1004:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='/root/myprojectishere/myproject/tensorboard_logs/', job_name='my_deepspeed_run_1') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-04-05 13:35:38,765] [INFO] [config.py:1004:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-04-05 13:35:38,765] [INFO] [config.py:1004:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-04-05 13:35:38,766] [INFO] [config.py:1004:print]   optimizer_name ............... None\n",
      "[2025-04-05 13:35:38,766] [INFO] [config.py:1004:print]   optimizer_params ............. None\n",
      "[2025-04-05 13:35:38,766] [INFO] [config.py:1004:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-04-05 13:35:38,767] [INFO] [config.py:1004:print]   pld_enabled .................. False\n",
      "[2025-04-05 13:35:38,767] [INFO] [config.py:1004:print]   pld_params ................... False\n",
      "[2025-04-05 13:35:38,768] [INFO] [config.py:1004:print]   prescale_gradients ........... False\n",
      "[2025-04-05 13:35:38,768] [INFO] [config.py:1004:print]   scheduler_name ............... None\n",
      "[2025-04-05 13:35:38,768] [INFO] [config.py:1004:print]   scheduler_params ............. None\n",
      "[2025-04-05 13:35:38,769] [INFO] [config.py:1004:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-04-05 13:35:38,769] [INFO] [config.py:1004:print]   sparse_attention ............. None\n",
      "[2025-04-05 13:35:38,770] [INFO] [config.py:1004:print]   sparse_gradients_enabled ..... False\n",
      "[2025-04-05 13:35:38,770] [INFO] [config.py:1004:print]   steps_per_print .............. 50\n",
      "[2025-04-05 13:35:38,770] [INFO] [config.py:1004:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n",
      "[2025-04-05 13:35:38,771] [INFO] [config.py:1004:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-04-05 13:35:38,771] [INFO] [config.py:1004:print]   train_batch_size ............. 256\n",
      "[2025-04-05 13:35:38,772] [INFO] [config.py:1004:print]   train_micro_batch_size_per_gpu  64\n",
      "[2025-04-05 13:35:38,772] [INFO] [config.py:1004:print]   use_data_before_expert_parallel_  False\n",
      "[2025-04-05 13:35:38,772] [INFO] [config.py:1004:print]   use_node_local_storage ....... False\n",
      "[2025-04-05 13:35:38,773] [INFO] [config.py:1004:print]   wall_clock_breakdown ......... False\n",
      "[2025-04-05 13:35:38,773] [INFO] [config.py:1004:print]   weight_quantization_config ... None\n",
      "[2025-04-05 13:35:38,774] [INFO] [config.py:1004:print]   world_size ................... 1\n",
      "[2025-04-05 13:35:38,774] [INFO] [config.py:1004:print]   zero_allow_untested_optimizer  False\n",
      "[2025-04-05 13:35:38,775] [INFO] [config.py:1004:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n",
      "[2025-04-05 13:35:38,775] [INFO] [config.py:1004:print]   zero_enabled ................. True\n",
      "[2025-04-05 13:35:38,775] [INFO] [config.py:1004:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-04-05 13:35:38,776] [INFO] [config.py:1004:print]   zero_optimization_stage ...... 2\n",
      "[2025-04-05 13:35:38,776] [INFO] [config.py:990:print_user_config]   json = {\n",
      "    \"train_batch_size\": 256, \n",
      "    \"train_micro_batch_size_per_gpu\": 64, \n",
      "    \"gradient_accumulation_steps\": 4, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_bucket_size\": 5.000000e+08, \n",
      "        \"allgather_bucket_size\": 5.000000e+08, \n",
      "        \"reduce_scatter\": true\n",
      "    }, \n",
      "    \"gradient_clipping\": 1, \n",
      "    \"steps_per_print\": 50, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"dump_state\": false, \n",
      "    \"tensorboard\": {\n",
      "        \"enabled\": false, \n",
      "        \"output_path\": \"/root/myprojectishere/myproject/tensorboard_logs/\", \n",
      "        \"job_name\": \"my_deepspeed_run_1\"\n",
      "    }, \n",
      "    \"flops_profiler\": {\n",
      "        \"enabled\": false, \n",
      "        \"profile_step\": 10, \n",
      "        \"module_depth\": -1, \n",
      "        \"top_modules\": 3, \n",
      "        \"detailed\": true\n",
      "    }, \n",
      "    \"timing\": {\n",
      "        \"enabled\": false\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import deepspeed\n",
    "from deepspeed.ops.adam import FusedAdam\n",
    "from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR\n",
    "\n",
    "#Setting up the linear Lr with cosine annealing deepspeed didnt provide me with it\n",
    "\n",
    "learning_rate = 1e-4\n",
    "num_gradient_steps_per_epoch = len(dataloader_train) // 4 \n",
    "num_training_steps = num_gradient_steps_per_epoch * 3 # TOTAL training steps<-check this\n",
    "#probably have to write this like get the num batch from len of dataloader then int divide from gradstep since we have to see when .step will be called and then for total number \n",
    "#epochs\n",
    "num_warmup_steps = int(0.1*(num_training_steps))\n",
    "\n",
    "eta_min = 0 # Minimum LR for cosine decay\n",
    "\n",
    "# Model Intialization\n",
    "\n",
    "model=ImageEncoder(5,512,512,8,2048,3,49408,'vilbert')\n",
    "\n",
    "#Setting up the optimizer\n",
    "optimizer=FusedAdam(model.parameters(),lr=1e-4,betas=(0.9,0.999),eps=1e-8,weight_decay=0.01)\n",
    "\n",
    "# 1. Warmup Scheduler\n",
    "# Starts from factor 1e-4 (approx 0) and goes to 1.0 over num_warmup_steps\n",
    "scheduler_warmup = LinearLR(optimizer, start_factor=1e-4, end_factor=1.0, total_iters=num_warmup_steps)\n",
    "\n",
    "# 2. Cosine Decay Scheduler\n",
    "# Starts decaying AFTER warmup is finished\n",
    "num_cosine_steps = num_training_steps - num_warmup_steps\n",
    "scheduler_cosine = CosineAnnealingLR(optimizer, T_max=num_cosine_steps, eta_min=eta_min)\n",
    "\n",
    "# 3. Chain them\n",
    "# The scheduler_cosine starts at the step specified in milestones\n",
    "scheduler = SequentialLR(optimizer, schedulers=[scheduler_warmup, scheduler_cosine], milestones=[num_warmup_steps])\n",
    "\n",
    "# Initialize DeepSpeed\n",
    "model_engine= deepspeed.initialize(\n",
    "    model=model,\n",
    "    optimizer=optimizer, \n",
    "    lr_scheduler=scheduler,\n",
    "    config_params='/root/myprojectishere/myproject/ds_config.json'\n",
    ")\n",
    "\n",
    "\n",
    "# Extract output_path and job_name from the loaded config\n",
    "# Create the TensorBoard writer\n",
    "#writer = SummaryWriter(log_dir='/root/myprojectishere/myproject/tensor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_deepspeed(model_engine,criteria, dataloader,epoch, total_epochs): \n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using the DeepSpeed engine and logs to TensorBoard.\n",
    "    ... (rest of docstring) ...\n",
    "    Args:\n",
    "        ...\n",
    "        writer: A torch.utils.tensorboard.SummaryWriter instance (or None if not rank 0).\n",
    "    \"\"\"\n",
    "    num_batches = len(dataloader)\n",
    "    if epoch==0:\n",
    "        print(f\"The number of batches that have been selected---->:{num_batches}\")\n",
    "\n",
    "        \n",
    "    model_engine.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_ce_loss = 0\n",
    "    total_clip_loss = 0\n",
    "    total_aux_loss = 0\n",
    "\n",
    "    # Adaptive teacher forcing schedule\n",
    "    min_teacher_forcing = 0.3\n",
    "    teacher_forcing_ratio = max(\n",
    "        min_teacher_forcing,\n",
    "        1.0 - (epoch / (total_epochs * 0.75))\n",
    "    )\n",
    "    print(f\"Epoch {epoch+1}/{total_epochs} using teacher forcing ratio: {teacher_forcing_ratio:.3f}\")\n",
    "\n",
    "    for batch_idx, (images, captions) in enumerate(dataloader):\n",
    "        images = images.to(model_engine.local_rank).to(dtype=torch.bfloat16)\n",
    "        captions = captions.to(model_engine.local_rank).to(dtype=torch.long)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs, aux_loss, clip_loss = model_engine(images, captions, None, teacher_forcing_ratio)\n",
    "\n",
    "        # Calculate losses\n",
    "        targets = captions[:, 1:]\n",
    "        ce_loss = criteria(outputs.reshape(-1, outputs.shape[-1]), targets.reshape(-1))\n",
    "\n",
    "        alpha = min(0.2, 0.05 + (epoch / total_epochs) * 0.15)\n",
    "        beta = 0.1\n",
    "        combined_loss = ce_loss + alpha * clip_loss\n",
    "        if aux_loss is not None:\n",
    "            combined_loss += beta * aux_loss\n",
    "\n",
    "        # Backward pass\n",
    "        model_engine.backward(combined_loss)\n",
    "\n",
    "        # Optimizer step \n",
    "        model_engine.step()\n",
    "        \n",
    "        # # --- OR ---# If you didn't pass lr_scheduler to initialize:\n",
    "        # model_engine.step() # Only steps the optimizer\n",
    "        # lr_scheduler_engine.step() # Manually step the scheduler AFTER the optimizer step\n",
    "\n",
    "\n",
    "        # Accumulate for epoch average logging (optional, could just rely on step logs)\n",
    "        current_lr = model_engine.get_lr()[0]\n",
    "        total_loss += combined_loss.item()\n",
    "        total_ce_loss += ce_loss.item()\n",
    "        total_clip_loss += clip_loss.item() if torch.is_tensor(clip_loss) else clip_loss\n",
    "        if aux_loss is not None:\n",
    "         total_aux_loss += aux_loss.item() if torch.is_tensor(aux_loss) else aux_loss\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        #Print progress to console \n",
    "        if  (batch_idx + 1) % 32 == 0:\n",
    "             print(f\"Epoch: {epoch+1}/{total_epochs}, Batch: {batch_idx+1}/{len(dataloader)}, \"\n",
    "                   f\"Global Step: {model_engine.global_steps}, \"\n",
    "                   f\"Step Loss: {combined_loss.item():.4f}, \"\n",
    "                   f\"LR: {current_lr:.6e}\",\n",
    "                   f\"Aux_loss:{aux_loss.item():.4f}\",\n",
    "                   f\"Clip_loss:{clip_loss.item():.4f}\",\n",
    "                   f\"Ce_loss:{ce_loss.item():.4f}\"\n",
    "                   )\n",
    "        \"\"\"if(batch_idx+1)==200:\n",
    "            torch.cuda.empty_cache()\n",
    "            return model_engine\"\"\"\n",
    "\n",
    "    # --- End of epoch ---#\n",
    "    num_batches = len(dataloader)\n",
    "    #if writer is not None: # Log average epoch losses if rank 0\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_ce_loss = total_ce_loss / num_batches\n",
    "    avg_clip_loss = total_clip_loss / num_batches\n",
    "    avg_aux_loss = total_aux_loss / num_batches if total_aux_loss > 0 else 0\n",
    "\n",
    "        #writer.add_scalar('Loss/train_epoch_combined', avg_loss, epoch + 1) \n",
    "        #writer.add_scalar('Loss/train_epoch_ce', avg_ce_loss, epoch + 1)\n",
    "        #writer.add_scalar('Loss/train_epoch_clip', avg_clip_loss, epoch + 1)\n",
    "        #writer.add_scalar('Loss/train_epoch_aux', avg_aux_loss, epoch + 1)\n",
    "\n",
    "    print(f\"\\n--- Epoch {epoch+1} Summary ---\")\n",
    "    print(f\"Avg Loss: {avg_loss:.4f}\")\n",
    "    print(f\"avg_clip_loss:{avg_clip_loss}\")\n",
    "    print(f\"avg_ce_loss:{avg_ce_loss:.4f}\")\n",
    "        # ... (rest of the print summary) ...\n",
    "    print(\"\")\n",
    "    return model_engine \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_for_validation(base_model, dataloader, criteria):\n",
    "    base_model.eval()\n",
    "    teacher_forcing = 1.0  # Full teacher forcing for evaluation\n",
    "\n",
    "    total_loss = 0\n",
    "    total_batches = len(dataloader)\n",
    "\n",
    "    with torch.no_grad():  # <-- Important context for disabling gradients\n",
    "        for batch_idx, (images, captions) in enumerate(dataloader):\n",
    "            images = images.to('cuda').to(dtype=torch.bfloat16)\n",
    "            captions = captions.to('cuda').to(dtype=torch.long)\n",
    "\n",
    "            outputs, aux_loss, siglip_loss = base_model(images, captions, None, teacher_forcing)\n",
    "            \n",
    "            targets = captions[:, 1:]\n",
    "            ce_loss = criteria(outputs.reshape(-1, outputs.shape[-1]), targets.reshape(-1))\n",
    "            \n",
    "            combined_loss = ce_loss + siglip_loss\n",
    "            if aux_loss is not None:\n",
    "                combined_loss += aux_loss\n",
    "\n",
    "            total_loss += combined_loss.item()\n",
    "\n",
    "            if (batch_idx + 1) % 8 == 0:\n",
    "                print(f\"Batch {batch_idx+1}/{total_batches}, \"\n",
    "                      f\"Combined Loss: {combined_loss.item():.4f}, \"\n",
    "                      f\"CE: {ce_loss.item():.4f}, \"\n",
    "                      f\"AUX: {aux_loss.item() if aux_loss is not None else 0:.4f}, \"\n",
    "                      f\"CLIP: {siglip_loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / total_batches\n",
    "    print(f\"\\nValidation Avg Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of batches that have been selected---->:1849\n",
      "Epoch 1/3 using teacher forcing ratio: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3, Batch: 32/1849, Global Step: 8, Step Loss: 10.9158, LR: 5.806522e-06 Aux_loss:0.0474 Clip_loss:0.7216 Ce_loss:10.8750\n",
      "Epoch: 1/3, Batch: 64/1849, Global Step: 16, Step Loss: 10.0865, LR: 1.160304e-05 Aux_loss:0.0475 Clip_loss:0.3845 Ce_loss:10.0625\n",
      "Epoch: 1/3, Batch: 96/1849, Global Step: 24, Step Loss: 9.2639, LR: 1.739957e-05 Aux_loss:0.0486 Clip_loss:0.1818 Ce_loss:9.2500\n",
      "Epoch: 1/3, Batch: 128/1849, Global Step: 32, Step Loss: 8.8223, LR: 2.319609e-05 Aux_loss:0.0478 Clip_loss:0.1002 Ce_loss:8.8125\n",
      "Epoch: 1/3, Batch: 160/1849, Global Step: 40, Step Loss: 8.2588, LR: 2.899261e-05 Aux_loss:0.0473 Clip_loss:0.0812 Ce_loss:8.2500\n",
      "Epoch: 1/3, Batch: 192/1849, Global Step: 48, Step Loss: 7.6338, LR: 3.478913e-05 Aux_loss:0.0477 Clip_loss:0.0803 Ce_loss:7.6250\n",
      "[2025-04-05 13:55:19,535] [INFO] [logging.py:107:log_dist] [Rank 0] step=50, skipped=0, lr=[3.6238260869565215e-05], mom=[(0.9, 0.999)]\n",
      "[2025-04-05 13:55:19,546] [INFO] [timer.py:264:stop] epoch=0/micro_step=200/global_step=50, RunningAvgSamplesPerSec=10.880104800939733, CurrSamplesPerSec=10.98463276142228, MemAllocated=2.89GB, MaxMemAllocated=17.16GB\n",
      "Epoch: 1/3, Batch: 224/1849, Global Step: 56, Step Loss: 6.9149, LR: 4.058565e-05 Aux_loss:0.0471 Clip_loss:0.0782 Ce_loss:6.9062\n",
      "Epoch: 1/3, Batch: 256/1849, Global Step: 64, Step Loss: 6.6651, LR: 4.638217e-05 Aux_loss:0.0481 Clip_loss:0.0805 Ce_loss:6.6562\n",
      "Epoch: 1/3, Batch: 288/1849, Global Step: 72, Step Loss: 6.0085, LR: 5.217870e-05 Aux_loss:0.0479 Clip_loss:0.0749 Ce_loss:6.0000\n",
      "Epoch: 1/3, Batch: 320/1849, Global Step: 80, Step Loss: 5.4776, LR: 5.797522e-05 Aux_loss:0.0484 Clip_loss:0.0812 Ce_loss:5.4688\n",
      "Epoch: 1/3, Batch: 352/1849, Global Step: 88, Step Loss: 5.4462, LR: 6.377174e-05 Aux_loss:0.0484 Clip_loss:0.0780 Ce_loss:5.4375\n",
      "Epoch: 1/3, Batch: 384/1849, Global Step: 96, Step Loss: 5.0712, LR: 6.956826e-05 Aux_loss:0.0487 Clip_loss:0.0762 Ce_loss:5.0625\n",
      "[2025-04-05 14:14:53,737] [INFO] [logging.py:107:log_dist] [Rank 0] step=100, skipped=0, lr=[7.246652173913041e-05], mom=[(0.9, 0.999)]\n",
      "[2025-04-05 14:14:53,749] [INFO] [timer.py:264:stop] epoch=0/micro_step=400/global_step=100, RunningAvgSamplesPerSec=10.894372082342075, CurrSamplesPerSec=10.735365402568476, MemAllocated=2.89GB, MaxMemAllocated=17.16GB\n",
      "Epoch: 1/3, Batch: 416/1849, Global Step: 104, Step Loss: 5.2589, LR: 7.536478e-05 Aux_loss:0.0488 Clip_loss:0.0805 Ce_loss:5.2500\n",
      "Epoch: 1/3, Batch: 448/1849, Global Step: 112, Step Loss: 4.7902, LR: 8.116130e-05 Aux_loss:0.0492 Clip_loss:0.0797 Ce_loss:4.7812\n",
      "Epoch: 1/3, Batch: 480/1849, Global Step: 120, Step Loss: 4.8524, LR: 8.695783e-05 Aux_loss:0.0486 Clip_loss:0.0750 Ce_loss:4.8438\n",
      "Epoch: 1/3, Batch: 512/1849, Global Step: 128, Step Loss: 4.6337, LR: 9.275435e-05 Aux_loss:0.0495 Clip_loss:0.0749 Ce_loss:4.6250\n",
      "Epoch: 1/3, Batch: 544/1849, Global Step: 136, Step Loss: 4.1963, LR: 9.855087e-05 Aux_loss:0.0493 Clip_loss:0.0776 Ce_loss:4.1875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:243: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3, Batch: 576/1849, Global Step: 144, Step Loss: 4.1961, LR: 9.999430e-05 Aux_loss:0.0493 Clip_loss:0.0732 Ce_loss:4.1875\n",
      "[2025-04-05 14:34:47,485] [INFO] [logging.py:107:log_dist] [Rank 0] step=150, skipped=0, lr=[9.99771892244767e-05], mom=[(0.9, 0.999)]\n",
      "[2025-04-05 14:34:47,497] [INFO] [timer.py:264:stop] epoch=0/micro_step=600/global_step=150, RunningAvgSamplesPerSec=10.838072268088874, CurrSamplesPerSec=10.698158225140014, MemAllocated=2.89GB, MaxMemAllocated=17.16GB\n",
      "Epoch: 1/3, Batch: 608/1849, Global Step: 152, Step Loss: 3.9618, LR: 9.996895e-05 Aux_loss:0.0490 Clip_loss:0.0763 Ce_loss:3.9531\n",
      "Epoch: 1/3, Batch: 640/1849, Global Step: 160, Step Loss: 3.8991, LR: 9.992334e-05 Aux_loss:0.0483 Clip_loss:0.0727 Ce_loss:3.8906\n",
      "Epoch: 1/3, Batch: 672/1849, Global Step: 168, Step Loss: 4.0398, LR: 9.985749e-05 Aux_loss:0.0485 Clip_loss:0.0729 Ce_loss:4.0312\n",
      "Epoch: 1/3, Batch: 704/1849, Global Step: 176, Step Loss: 3.7274, LR: 9.977142e-05 Aux_loss:0.0486 Clip_loss:0.0768 Ce_loss:3.7188\n",
      "Epoch: 1/3, Batch: 736/1849, Global Step: 184, Step Loss: 3.4774, LR: 9.966516e-05 Aux_loss:0.0488 Clip_loss:0.0761 Ce_loss:3.4688\n",
      "Epoch: 1/3, Batch: 768/1849, Global Step: 192, Step Loss: 3.4618, LR: 9.953876e-05 Aux_loss:0.0494 Clip_loss:0.0740 Ce_loss:3.4531\n",
      "[2025-04-05 14:54:39,991] [INFO] [logging.py:107:log_dist] [Rank 0] step=200, skipped=0, lr=[9.93922678484877e-05], mom=[(0.9, 0.999)]\n",
      "[2025-04-05 14:54:40,002] [INFO] [timer.py:264:stop] epoch=0/micro_step=800/global_step=200, RunningAvgSamplesPerSec=10.813280264209647, CurrSamplesPerSec=10.72048489166873, MemAllocated=2.89GB, MaxMemAllocated=17.16GB\n",
      "Epoch: 1/3, Batch: 800/1849, Global Step: 200, Step Loss: 3.3367, LR: 9.939227e-05 Aux_loss:0.0484 Clip_loss:0.0743 Ce_loss:3.3281\n",
      "Epoch: 1/3, Batch: 832/1849, Global Step: 208, Step Loss: 3.0868, LR: 9.922575e-05 Aux_loss:0.0490 Clip_loss:0.0760 Ce_loss:3.0781\n",
      "Epoch: 1/3, Batch: 864/1849, Global Step: 216, Step Loss: 2.9617, LR: 9.903926e-05 Aux_loss:0.0483 Clip_loss:0.0750 Ce_loss:2.9531\n",
      "Epoch: 1/3, Batch: 896/1849, Global Step: 224, Step Loss: 3.0868, LR: 9.883289e-05 Aux_loss:0.0487 Clip_loss:0.0752 Ce_loss:3.0781\n",
      "Epoch: 1/3, Batch: 928/1849, Global Step: 232, Step Loss: 2.6181, LR: 9.860672e-05 Aux_loss:0.0485 Clip_loss:0.0766 Ce_loss:2.6094\n",
      "Epoch: 1/3, Batch: 960/1849, Global Step: 240, Step Loss: 2.5555, LR: 9.836083e-05 Aux_loss:0.0483 Clip_loss:0.0757 Ce_loss:2.5469\n",
      "Epoch: 1/3, Batch: 992/1849, Global Step: 248, Step Loss: 2.3212, LR: 9.809533e-05 Aux_loss:0.0483 Clip_loss:0.0778 Ce_loss:2.3125\n",
      "[2025-04-05 15:14:40,574] [INFO] [logging.py:107:log_dist] [Rank 0] step=250, skipped=0, lr=[9.802590558156863e-05], mom=[(0.9, 0.999)]\n",
      "[2025-04-05 15:14:40,586] [INFO] [timer.py:264:stop] epoch=0/micro_step=1000/global_step=250, RunningAvgSamplesPerSec=10.783717085722024, CurrSamplesPerSec=10.62807078170809, MemAllocated=2.89GB, MaxMemAllocated=17.17GB\n",
      "Epoch: 1/3, Batch: 1024/1849, Global Step: 256, Step Loss: 2.3836, LR: 9.781033e-05 Aux_loss:0.0474 Clip_loss:0.0764 Ce_loss:2.3750\n",
      "Epoch: 1/3, Batch: 1056/1849, Global Step: 264, Step Loss: 1.9306, LR: 9.750594e-05 Aux_loss:0.0481 Clip_loss:0.0782 Ce_loss:1.9219\n",
      "Epoch: 1/3, Batch: 1088/1849, Global Step: 272, Step Loss: 1.9382, LR: 9.718228e-05 Aux_loss:0.0485 Clip_loss:0.0740 Ce_loss:1.9297\n",
      "Epoch: 1/3, Batch: 1120/1849, Global Step: 280, Step Loss: 1.7508, LR: 9.683948e-05 Aux_loss:0.0490 Clip_loss:0.0740 Ce_loss:1.7422\n",
      "Epoch: 1/3, Batch: 1152/1849, Global Step: 288, Step Loss: 1.6336, LR: 9.647770e-05 Aux_loss:0.0482 Clip_loss:0.0764 Ce_loss:1.6250\n",
      "Epoch: 1/3, Batch: 1184/1849, Global Step: 296, Step Loss: 1.4775, LR: 9.609706e-05 Aux_loss:0.0483 Clip_loss:0.0777 Ce_loss:1.4688\n",
      "[2025-04-05 15:34:39,494] [INFO] [logging.py:107:log_dist] [Rank 0] step=300, skipped=0, lr=[9.589971980218187e-05], mom=[(0.9, 0.999)]\n",
      "[2025-04-05 15:34:39,506] [INFO] [timer.py:264:stop] epoch=0/micro_step=1200/global_step=300, RunningAvgSamplesPerSec=10.766683241203497, CurrSamplesPerSec=10.667204969411731, MemAllocated=2.89GB, MaxMemAllocated=17.17GB\n",
      "Epoch: 1/3, Batch: 1216/1849, Global Step: 304, Step Loss: 1.4852, LR: 9.569773e-05 Aux_loss:0.0485 Clip_loss:0.0753 Ce_loss:1.4766\n",
      "Epoch: 1/3, Batch: 1248/1849, Global Step: 312, Step Loss: 1.2430, LR: 9.527986e-05 Aux_loss:0.0478 Clip_loss:0.0759 Ce_loss:1.2344\n",
      "Epoch: 1/3, Batch: 1280/1849, Global Step: 320, Step Loss: 1.1882, LR: 9.484364e-05 Aux_loss:0.0477 Clip_loss:0.0748 Ce_loss:1.1797\n",
      "Epoch: 1/3, Batch: 1312/1849, Global Step: 328, Step Loss: 1.2117, LR: 9.438922e-05 Aux_loss:0.0483 Clip_loss:0.0742 Ce_loss:1.2031\n",
      "Epoch: 1/3, Batch: 1344/1849, Global Step: 336, Step Loss: 0.9854, LR: 9.391681e-05 Aux_loss:0.0487 Clip_loss:0.0786 Ce_loss:0.9766\n",
      "Epoch: 1/3, Batch: 1376/1849, Global Step: 344, Step Loss: 1.0790, LR: 9.342659e-05 Aux_loss:0.0486 Clip_loss:0.0765 Ce_loss:1.0703\n",
      "[2025-04-05 15:54:39,870] [INFO] [logging.py:107:log_dist] [Rank 0] step=350, skipped=0, lr=[9.30473491461321e-05], mom=[(0.9, 0.999)]\n",
      "[2025-04-05 15:54:39,881] [INFO] [timer.py:264:stop] epoch=0/micro_step=1400/global_step=350, RunningAvgSamplesPerSec=10.752686469259313, CurrSamplesPerSec=10.681288581812831, MemAllocated=2.89GB, MaxMemAllocated=17.17GB\n",
      "Epoch: 1/3, Batch: 1408/1849, Global Step: 352, Step Loss: 0.8251, LR: 9.291875e-05 Aux_loss:0.0491 Clip_loss:0.0751 Ce_loss:0.8164\n",
      "Epoch: 1/3, Batch: 1440/1849, Global Step: 360, Step Loss: 0.8563, LR: 9.239351e-05 Aux_loss:0.0483 Clip_loss:0.0759 Ce_loss:0.8477\n",
      "Epoch: 1/3, Batch: 1472/1849, Global Step: 368, Step Loss: 0.7897, LR: 9.185108e-05 Aux_loss:0.0477 Clip_loss:0.0739 Ce_loss:0.7812\n",
      "Epoch: 1/3, Batch: 1504/1849, Global Step: 376, Step Loss: 0.6139, LR: 9.129167e-05 Aux_loss:0.0469 Clip_loss:0.0745 Ce_loss:0.6055\n",
      "Epoch: 1/3, Batch: 1536/1849, Global Step: 384, Step Loss: 0.7157, LR: 9.071552e-05 Aux_loss:0.0477 Clip_loss:0.0777 Ce_loss:0.7070\n",
      "Epoch: 1/3, Batch: 1568/1849, Global Step: 392, Step Loss: 0.6531, LR: 9.012286e-05 Aux_loss:0.0487 Clip_loss:0.0745 Ce_loss:0.6445\n",
      "[2025-04-05 16:14:37,075] [INFO] [logging.py:107:log_dist] [Rank 0] step=400, skipped=0, lr=[8.951392130574485e-05], mom=[(0.9, 0.999)]\n",
      "[2025-04-05 16:14:37,086] [INFO] [timer.py:264:stop] epoch=0/micro_step=1600/global_step=400, RunningAvgSamplesPerSec=10.745823998555528, CurrSamplesPerSec=10.723077068662233, MemAllocated=2.89GB, MaxMemAllocated=17.17GB\n",
      "Epoch: 1/3, Batch: 1600/1849, Global Step: 400, Step Loss: 0.6376, LR: 8.951392e-05 Aux_loss:0.0477 Clip_loss:0.0792 Ce_loss:0.6289\n",
      "Epoch: 1/3, Batch: 1632/1849, Global Step: 408, Step Loss: 0.7351, LR: 8.888896e-05 Aux_loss:0.0486 Clip_loss:0.0745 Ce_loss:0.7266\n",
      "Epoch: 1/3, Batch: 1664/1849, Global Step: 416, Step Loss: 0.4812, LR: 8.824823e-05 Aux_loss:0.0484 Clip_loss:0.0735 Ce_loss:0.4727\n",
      "Epoch: 1/3, Batch: 1696/1849, Global Step: 424, Step Loss: 0.6062, LR: 8.759199e-05 Aux_loss:0.0473 Clip_loss:0.0755 Ce_loss:0.5977\n",
      "Epoch: 1/3, Batch: 1728/1849, Global Step: 432, Step Loss: 0.4949, LR: 8.692050e-05 Aux_loss:0.0479 Clip_loss:0.0766 Ce_loss:0.4863\n",
      "Epoch: 1/3, Batch: 1760/1849, Global Step: 440, Step Loss: 0.5124, LR: 8.623404e-05 Aux_loss:0.0480 Clip_loss:0.0748 Ce_loss:0.5039\n",
      "Epoch: 1/3, Batch: 1792/1849, Global Step: 448, Step Loss: 0.5400, LR: 8.553289e-05 Aux_loss:0.0478 Clip_loss:0.0785 Ce_loss:0.5312\n",
      "[2025-04-05 16:34:37,038] [INFO] [logging.py:107:log_dist] [Rank 0] step=450, skipped=0, lr=[8.535533905932746e-05], mom=[(0.9, 0.999)]\n",
      "[2025-04-05 16:34:37,049] [INFO] [timer.py:264:stop] epoch=0/micro_step=1800/global_step=450, RunningAvgSamplesPerSec=10.737729149030057, CurrSamplesPerSec=10.69442300264585, MemAllocated=2.89GB, MaxMemAllocated=17.17GB\n",
      "Epoch: 1/3, Batch: 1824/1849, Global Step: 456, Step Loss: 0.5438, LR: 8.481733e-05 Aux_loss:0.0481 Clip_loss:0.0770 Ce_loss:0.5352\n",
      "\n",
      "--- Epoch 1 Summary ---\n",
      "Avg Loss: 3.3465\n",
      "avg_clip_loss:0.10064082768815734\n",
      "avg_ce_loss:3.3366\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 8/157, Combined Loss: 0.5434, CE: 0.3574, AUX: 0.0479, CLIP: 0.1381\n",
      "Batch 16/157, Combined Loss: 0.8074, CE: 0.6211, AUX: 0.0482, CLIP: 0.1381\n",
      "Batch 24/157, Combined Loss: 0.5164, CE: 0.3301, AUX: 0.0477, CLIP: 0.1386\n",
      "Batch 32/157, Combined Loss: 0.4254, CE: 0.2412, AUX: 0.0478, CLIP: 0.1364\n",
      "Batch 40/157, Combined Loss: 0.5312, CE: 0.3457, AUX: 0.0483, CLIP: 0.1372\n",
      "Batch 48/157, Combined Loss: 0.5663, CE: 0.3809, AUX: 0.0480, CLIP: 0.1374\n",
      "Batch 56/157, Combined Loss: 0.9729, CE: 0.7852, AUX: 0.0487, CLIP: 0.1390\n",
      "Batch 64/157, Combined Loss: 0.6917, CE: 0.5078, AUX: 0.0474, CLIP: 0.1365\n",
      "Batch 72/157, Combined Loss: 0.7278, CE: 0.5430, AUX: 0.0477, CLIP: 0.1371\n",
      "Batch 80/157, Combined Loss: 0.4223, CE: 0.2344, AUX: 0.0474, CLIP: 0.1405\n",
      "Batch 88/157, Combined Loss: 0.6597, CE: 0.4746, AUX: 0.0484, CLIP: 0.1366\n",
      "Batch 96/157, Combined Loss: 0.8014, CE: 0.6133, AUX: 0.0477, CLIP: 0.1404\n",
      "Batch 104/157, Combined Loss: 0.5947, CE: 0.4102, AUX: 0.0475, CLIP: 0.1371\n",
      "Batch 112/157, Combined Loss: 0.4522, CE: 0.2676, AUX: 0.0477, CLIP: 0.1369\n",
      "Batch 120/157, Combined Loss: 0.5965, CE: 0.4102, AUX: 0.0475, CLIP: 0.1389\n",
      "Batch 128/157, Combined Loss: 0.8523, CE: 0.6680, AUX: 0.0476, CLIP: 0.1367\n",
      "Batch 136/157, Combined Loss: 0.6086, CE: 0.4238, AUX: 0.0477, CLIP: 0.1371\n",
      "Batch 144/157, Combined Loss: 0.4937, CE: 0.3125, AUX: 0.0477, CLIP: 0.1334\n",
      "Batch 152/157, Combined Loss: 0.6272, CE: 0.4395, AUX: 0.0477, CLIP: 0.1401\n",
      "\n",
      "Validation Avg Loss: 0.6061\n",
      "[Epoch 1/3] Validation Loss: 0.6061\n",
      "Epoch 2/3 using teacher forcing ratio: 0.556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"/pytorch/c10/cuda/CUDACachingAllocator.cpp\":1015, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m model_engine \u001b[39m=\u001b[39m model_engine[\u001b[39m0\u001b[39m]  \u001b[39m# If needed, unpack just once\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(total_epochs):\n\u001b[0;32m----> 6\u001b[0m     model_engine \u001b[39m=\u001b[39m train_epoch_deepspeed(\n\u001b[1;32m      7\u001b[0m         model_engine\u001b[39m=\u001b[39;49mmodel_engine,\n\u001b[1;32m      8\u001b[0m         dataloader\u001b[39m=\u001b[39;49mdataloader_train,\n\u001b[1;32m      9\u001b[0m         epoch\u001b[39m=\u001b[39;49mepoch,\n\u001b[1;32m     10\u001b[0m         criteria\u001b[39m=\u001b[39;49mcriteria,\n\u001b[1;32m     11\u001b[0m         total_epochs\u001b[39m=\u001b[39;49mtotal_epochs\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     14\u001b[0m     val_loss \u001b[39m=\u001b[39m model_for_validation(\n\u001b[1;32m     15\u001b[0m         model_engine,\n\u001b[1;32m     16\u001b[0m         dataloader_val,\n\u001b[1;32m     17\u001b[0m         criteria\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Epoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mtotal_epochs\u001b[39m}\u001b[39;00m\u001b[39m] Validation Loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 34\u001b[0m, in \u001b[0;36mtrain_epoch_deepspeed\u001b[0;34m(model_engine, criteria, dataloader, epoch, total_epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m captions \u001b[39m=\u001b[39m captions\u001b[39m.\u001b[39mto(model_engine\u001b[39m.\u001b[39mlocal_rank)\u001b[39m.\u001b[39mto(dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[1;32m     33\u001b[0m \u001b[39m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m outputs, aux_loss, clip_loss \u001b[39m=\u001b[39m model_engine(images, captions, \u001b[39mNone\u001b[39;49;00m, teacher_forcing_ratio)\n\u001b[1;32m     36\u001b[0m \u001b[39m# Calculate losses\u001b[39;00m\n\u001b[1;32m     37\u001b[0m targets \u001b[39m=\u001b[39m captions[:, \u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/deepspeed/utils/nvtx.py:20\u001b[0m, in \u001b[0;36minstrument_w_nvtx.<locals>.wrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mif\u001b[39;00m enable_nvtx \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_compiling():\n\u001b[1;32m     19\u001b[0m     get_accelerator()\u001b[39m.\u001b[39mrange_push(func\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m)\n\u001b[0;32m---> 20\u001b[0m ret_val \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     21\u001b[0m \u001b[39mif\u001b[39;00m enable_nvtx \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_compiling():\n\u001b[1;32m     22\u001b[0m     get_accelerator()\u001b[39m.\u001b[39mrange_pop()\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/deepspeed/runtime/engine.py:2030\u001b[0m, in \u001b[0;36mDeepSpeedEngine.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2027\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautotuning_profile_model_info():\n\u001b[1;32m   2028\u001b[0m     ma \u001b[39m=\u001b[39m get_ma_status()\n\u001b[0;32m-> 2030\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2032\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautotuning_profile_model_info():\n\u001b[1;32m   2033\u001b[0m     activation_mem \u001b[39m=\u001b[39m get_ma_status() \u001b[39m-\u001b[39m ma\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1845\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \u001b[39mreturn\u001b[39;00m inner()\n\u001b[1;32m   1844\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1845\u001b[0m     \u001b[39mreturn\u001b[39;00m inner()\n\u001b[1;32m   1846\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1847\u001b[0m     \u001b[39m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[1;32m   1848\u001b[0m     \u001b[39m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[1;32m   1849\u001b[0m     \u001b[39m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[1;32m   1850\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m _global_forward_hooks\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1793\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1790\u001b[0m     bw_hook \u001b[39m=\u001b[39m BackwardHook(\u001b[39mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1791\u001b[0m     args \u001b[39m=\u001b[39m bw_hook\u001b[39m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1793\u001b[0m result \u001b[39m=\u001b[39m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1794\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1795\u001b[0m     \u001b[39mfor\u001b[39;00m hook_id, hook \u001b[39min\u001b[39;00m (\n\u001b[1;32m   1796\u001b[0m         \u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1797\u001b[0m         \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mitems(),\n\u001b[1;32m   1798\u001b[0m     ):\n\u001b[1;32m   1799\u001b[0m         \u001b[39m# mark that always called hook is run\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 88\u001b[0m, in \u001b[0;36mImageEncoder.forward\u001b[0;34m(self, images, captions, attention_mask, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     accumulated_input \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([captions_embeds[:, :t], current_input], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m     step_output, step_aux_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m     89\u001b[0m         accumulated_input,\n\u001b[1;32m     90\u001b[0m         encoder_output\u001b[39m=\u001b[39;49mencoder_output)\n\u001b[1;32m     92\u001b[0m aux_losses\u001b[39m.\u001b[39mappend(step_aux_loss)\n\u001b[1;32m     93\u001b[0m decoder_output[:, t:t\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m step_output[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 18\u001b[0m, in \u001b[0;36mDecoderBlocks.forward\u001b[0;34m(self, x, encoder_output)\u001b[0m\n\u001b[1;32m     16\u001b[0m     encoder_output \u001b[39m=\u001b[39m encoder_output[:, :x\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m), :]\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m---> 18\u001b[0m     x, enhanced_x, enhanced_encoder, aux_loss \u001b[39m=\u001b[39m layer(x, encoder_output)\n\u001b[1;32m     19\u001b[0m     aux_loss_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m aux_loss\n\u001b[1;32m     20\u001b[0m     encoder_output \u001b[39m=\u001b[39m enhanced_encoder\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[8], line 39\u001b[0m, in \u001b[0;36mDecoderBlock.forward\u001b[0;34m(self, x, encoder_output)\u001b[0m\n\u001b[1;32m     36\u001b[0m aux_loss_total \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m     38\u001b[0m x_norm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x)\n\u001b[0;32m---> 39\u001b[0m self_attn_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(x_norm, causal_mask\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     40\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(self_attn_out)\n\u001b[1;32m     42\u001b[0m \u001b[39m# Only proceed with cross-attention if encoder output is provided\u001b[39;00m\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 56\u001b[0m, in \u001b[0;36mSelfAttention.forward\u001b[0;34m(self, x, causal_mask)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39m# Apply rotary position embeddings\u001b[39;00m\n\u001b[1;32m     55\u001b[0m rotary_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrotary_emb(seq_len, x\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m---> 56\u001b[0m q \u001b[39m=\u001b[39m apply_rotary_embedding(q, rotary_emb)\n\u001b[1;32m     57\u001b[0m k \u001b[39m=\u001b[39m apply_rotary_embedding(k, rotary_emb)\n\u001b[1;32m     59\u001b[0m \u001b[39m# Compute attention weights\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 30\u001b[0m, in \u001b[0;36mapply_rotary_embedding\u001b[0;34m(x, rotary_emb)\u001b[0m\n\u001b[1;32m     27\u001b[0m x_rot \u001b[39m=\u001b[39m x_1 \u001b[39m*\u001b[39m emb_cos \u001b[39m-\u001b[39m x_2 \u001b[39m*\u001b[39m emb_sin\n\u001b[1;32m     28\u001b[0m x_pass \u001b[39m=\u001b[39m x_1 \u001b[39m*\u001b[39m emb_sin \u001b[39m+\u001b[39m x_2 \u001b[39m*\u001b[39m emb_cos\n\u001b[0;32m---> 30\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mcat([x_rot, x_pass], dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"/pytorch/c10/cuda/CUDACachingAllocator.cpp\":1015, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "total_epochs = 3\n",
    "criteria = nn.CrossEntropyLoss(ignore_index=49407)\n",
    "model_engine = model_engine[0]  # If needed, unpack just once\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    model_engine = train_epoch_deepspeed(\n",
    "        model_engine=model_engine,\n",
    "        dataloader=dataloader_train,\n",
    "        epoch=epoch,\n",
    "        criteria=criteria,\n",
    "        total_epochs=total_epochs\n",
    "    )\n",
    "\n",
    "    val_loss = model_for_validation(\n",
    "        model_engine,\n",
    "        dataloader_val,\n",
    "        criteria\n",
    "    )\n",
    "    print(f\"[Epoch {epoch+1}/{total_epochs}] Validation Loss: {val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"/pytorch/c10/cuda/CUDACachingAllocator.cpp\":1015, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m/root/myprojectishere/myproject/model_weights_maybe_this_works.pth\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/serialization.py:1462\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1460\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m   1461\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1462\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(\n\u001b[1;32m   1463\u001b[0m             opened_zipfile,\n\u001b[1;32m   1464\u001b[0m             map_location,\n\u001b[1;32m   1465\u001b[0m             _weights_only_unpickler,\n\u001b[1;32m   1466\u001b[0m             overall_storage\u001b[39m=\u001b[39;49moverall_storage,\n\u001b[1;32m   1467\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args,\n\u001b[1;32m   1468\u001b[0m         )\n\u001b[1;32m   1469\u001b[0m     \u001b[39mexcept\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1470\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[39mstr\u001b[39m(e))) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/serialization.py:1964\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1962\u001b[0m \u001b[39mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   1963\u001b[0m _serialization_tls\u001b[39m.\u001b[39mmap_location \u001b[39m=\u001b[39m map_location\n\u001b[0;32m-> 1964\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1965\u001b[0m _serialization_tls\u001b[39m.\u001b[39mmap_location \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1967\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/_weights_only_unpickler.py:512\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    505\u001b[0m         \u001b[39mtype\u001b[39m(pid) \u001b[39mis\u001b[39;00m \u001b[39mtuple\u001b[39m\n\u001b[1;32m    506\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(pid) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    507\u001b[0m         \u001b[39mand\u001b[39;00m torch\u001b[39m.\u001b[39mserialization\u001b[39m.\u001b[39m_maybe_decode_ascii(pid[\u001b[39m0\u001b[39m]) \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mstorage\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m     ):\n\u001b[1;32m    509\u001b[0m         \u001b[39mraise\u001b[39;00m UnpicklingError(\n\u001b[1;32m    510\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[39m{\u001b[39;00mpid[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m         )\n\u001b[0;32m--> 512\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpersistent_load(pid))\n\u001b[1;32m    513\u001b[0m \u001b[39melif\u001b[39;00m key[\u001b[39m0\u001b[39m] \u001b[39min\u001b[39;00m [BINGET[\u001b[39m0\u001b[39m], LONG_BINGET[\u001b[39m0\u001b[39m]]:\n\u001b[1;32m    514\u001b[0m     idx \u001b[39m=\u001b[39m (read(\u001b[39m1\u001b[39m) \u001b[39mif\u001b[39;00m key[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m BINGET[\u001b[39m0\u001b[39m] \u001b[39melse\u001b[39;00m unpack(\u001b[39m\"\u001b[39m\u001b[39m<I\u001b[39m\u001b[39m\"\u001b[39m, read(\u001b[39m4\u001b[39m)))[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/serialization.py:1928\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1926\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1928\u001b[0m     typed_storage \u001b[39m=\u001b[39m load_tensor(\n\u001b[1;32m   1929\u001b[0m         dtype, nbytes, key, _maybe_decode_ascii(location)\n\u001b[1;32m   1930\u001b[0m     )\n\u001b[1;32m   1932\u001b[0m \u001b[39mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/serialization.py:1900\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1895\u001b[0m         storage\u001b[39m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1897\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1898\u001b[0m \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1899\u001b[0m typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1900\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1901\u001b[0m     dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   1902\u001b[0m     _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   1903\u001b[0m )\n\u001b[1;32m   1905\u001b[0m \u001b[39mif\u001b[39;00m typed_storage\u001b[39m.\u001b[39m_data_ptr() \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1906\u001b[0m     loaded_storages[key] \u001b[39m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/serialization.py:693\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[39mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39m       all matching ones return `None`.\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 693\u001b[0m     result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    694\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    695\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/serialization.py:632\u001b[0m, in \u001b[0;36m_deserialize\u001b[0;34m(backend_name, obj, location)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(backend_name):\n\u001b[1;32m    631\u001b[0m     device \u001b[39m=\u001b[39m _validate_device(location, backend_name)\n\u001b[0;32m--> 632\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39;49mto(device\u001b[39m=\u001b[39;49mdevice)\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/storage.py:292\u001b[0m, in \u001b[0;36m_StorageBase.to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(device, torch\u001b[39m.\u001b[39mdevice):\n\u001b[1;32m    291\u001b[0m     device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(device)\n\u001b[0;32m--> 292\u001b[0m \u001b[39mreturn\u001b[39;00m _to(\u001b[39mself\u001b[39;49m, device, non_blocking)\n",
      "File \u001b[0;32m~/myprojectishere/myproject/.venv/lib/python3.10/site-packages/torch/_utils.py:99\u001b[0m, in \u001b[0;36m_to\u001b[0;34m(self, device, non_blocking)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[1;32m     97\u001b[0m         \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_sparse\n\u001b[1;32m     98\u001b[0m     ), \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msparse storage is not supported for \u001b[39m\u001b[39m{\u001b[39;00mdevice\u001b[39m.\u001b[39mtype\u001b[39m.\u001b[39mupper()\u001b[39m}\u001b[39;00m\u001b[39m tensors\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 99\u001b[0m     untyped_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mUntypedStorage(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize(), device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m    100\u001b[0m     untyped_storage\u001b[39m.\u001b[39mcopy_(\u001b[39mself\u001b[39m, non_blocking)\n\u001b[1;32m    101\u001b[0m     \u001b[39mreturn\u001b[39;00m untyped_storage\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NVML_SUCCESS == r INTERNAL ASSERT FAILED at \"/pytorch/c10/cuda/CUDACachingAllocator.cpp\":1015, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "torch.load('/root/myprojectishere/myproject/model_weights_maybe_this_works.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img,caption in dataloader_val:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1572, -0.3032, -0.4930,  ..., -1.2813, -1.5879, -1.5879],\n",
       "         [-0.1864, -0.4346, -0.5806,  ..., -1.4273, -1.5879, -1.3689],\n",
       "         [-0.2156, -0.4930, -0.6536,  ..., -1.4711, -1.5733, -1.4857],\n",
       "         ...,\n",
       "         [ 0.2077,  0.2661,  0.2953,  ..., -1.0331, -1.0039, -1.0331],\n",
       "         [ 0.2369,  0.2515,  0.2807,  ..., -1.0623, -1.0331, -1.0769],\n",
       "         [ 0.2807,  0.2661,  0.2807,  ..., -1.0623, -1.0477, -1.0915]],\n",
       "\n",
       "        [[ 0.1089, -0.0412, -0.3264,  ..., -1.3769, -1.5570, -1.5420],\n",
       "         [ 0.0939, -0.1913, -0.4314,  ..., -1.4369, -1.5570, -1.3319],\n",
       "         [ 0.0488, -0.3564, -0.5065,  ..., -1.4519, -1.5420, -1.3919],\n",
       "         ...,\n",
       "         [-0.1613, -0.1463, -0.1163,  ..., -1.1968, -1.1818, -1.1968],\n",
       "         [-0.1613, -0.1463, -0.1313,  ..., -1.1968, -1.2268, -1.2568],\n",
       "         [-0.1313, -0.1163, -0.1313,  ..., -1.2118, -1.2268, -1.2718]],\n",
       "\n",
       "        [[-0.5559, -0.5559, -0.4706,  ..., -1.1532, -1.2954, -1.2669],\n",
       "         [-0.5844, -0.6697, -0.3568,  ..., -1.2385, -1.2954, -1.1105],\n",
       "         [-0.6412, -0.7977, -0.6128,  ..., -1.2385, -1.2669, -1.1674],\n",
       "         ...,\n",
       "         [-0.3853, -0.3853, -0.3711,  ..., -1.1247, -1.0963, -1.0963],\n",
       "         [-0.3853, -0.3995, -0.3711,  ..., -1.1105, -1.0963, -1.1247],\n",
       "         [-0.4137, -0.4137, -0.3995,  ..., -1.1105, -1.1105, -1.1532]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1572, -0.3032, -0.4930,  ..., -1.2813, -1.5879, -1.5879],\n",
       "         [-0.1864, -0.4346, -0.5806,  ..., -1.4273, -1.5879, -1.3689],\n",
       "         [-0.2156, -0.4930, -0.6536,  ..., -1.4711, -1.5733, -1.4857],\n",
       "         ...,\n",
       "         [ 0.2077,  0.2661,  0.2953,  ..., -1.0331, -1.0039, -1.0331],\n",
       "         [ 0.2369,  0.2515,  0.2807,  ..., -1.0623, -1.0331, -1.0769],\n",
       "         [ 0.2807,  0.2661,  0.2807,  ..., -1.0623, -1.0477, -1.0915]],\n",
       "\n",
       "        [[ 0.1089, -0.0412, -0.3264,  ..., -1.3769, -1.5570, -1.5420],\n",
       "         [ 0.0939, -0.1913, -0.4314,  ..., -1.4369, -1.5570, -1.3319],\n",
       "         [ 0.0488, -0.3564, -0.5065,  ..., -1.4519, -1.5420, -1.3919],\n",
       "         ...,\n",
       "         [-0.1613, -0.1463, -0.1163,  ..., -1.1968, -1.1818, -1.1968],\n",
       "         [-0.1613, -0.1463, -0.1313,  ..., -1.1968, -1.2268, -1.2568],\n",
       "         [-0.1313, -0.1163, -0.1313,  ..., -1.2118, -1.2268, -1.2718]],\n",
       "\n",
       "        [[-0.5559, -0.5559, -0.4706,  ..., -1.1532, -1.2954, -1.2669],\n",
       "         [-0.5844, -0.6697, -0.3568,  ..., -1.2385, -1.2954, -1.1105],\n",
       "         [-0.6412, -0.7977, -0.6128,  ..., -1.2385, -1.2669, -1.1674],\n",
       "         ...,\n",
       "         [-0.3853, -0.3853, -0.3711,  ..., -1.1247, -1.0963, -1.0963],\n",
       "         [-0.3853, -0.3995, -0.3711,  ..., -1.1105, -1.0963, -1.1247],\n",
       "         [-0.4137, -0.4137, -0.3995,  ..., -1.1105, -1.1105, -1.1532]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img=img[0]\n",
    "img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('/root/myprojectishere/myproject/model_weights_maybe_this_works.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=model.beam_search_caption_generation(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 269,\n",
       " 269,\n",
       " 269,\n",
       " 269,\n",
       " 269,\n",
       " 269,\n",
       " 269,\n",
       " 269,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836,\n",
       " 836]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caption: nicole\n"
     ]
    }
   ],
   "source": [
    "token_ids = [] # example greedy output\n",
    "\n",
    "caption = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "print(\"Caption:\", caption)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
